{"cells":[{"cell_type":"code","source":["# --- Cell 1: Notebook Header, Logging Configuration, and Library Imports ---\n","\n","\"\"\"\n","Notebook: 01_ingest_earthquake_data.ipynb\n","\n","Purpose:\n","This script manages the initial ingestion phase of raw earthquake data.\n","It connects to the USGS API to extract seismic information and defines the\n","necessary data schema for subsequent processing. The objective is to prepare\n","the data for storage in the Bronze layer of the Lakehouse architecture,\n","serving as a raw, untransformed data source.\n","\n","Dependencies:\n","- Python 3.x\n","- requests library (for HTTP requests)\n","- pandas library (for data manipulation)\n","- pyspark library (for distributed processing and DataFrame operations)\n","\n","Execution Environment:\n","This script is designed to run within an Apache Spark environment,\n","specifically optimized for platforms like Azure Fabric where a SparkSession\n","('spark') is pre-initialized.\n","\"\"\"\n","\n","# Configures a basic logging system for effective monitoring in production environments.\n","# This setup allows capturing informational messages, warnings, and errors throughout\n","# the script's execution, facilitating debugging and operational oversight.\n","import logging\n","logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n","logger = logging.getLogger(__name__)\n","\n","# Standard Python libraries are imported first for consistency.\n","import json\n","from datetime import datetime, timedelta, timezone # Added timezone for explicit UTC for robust time handling.\n","\n","# Third-party libraries for data handling and HTTP requests are grouped.\n","import requests  # Used for making HTTP GET requests to the USGS API.\n","import pandas as pd  # Utilized for initial data structuring in a Pandas DataFrame.\n","\n","# PySpark libraries for distributed processing and DataFrame manipulation are imported last.\n","# SparkSession: The entry point for programming Spark with the DataFrame and SQL API.\n","# StructType, StructField: Essential for defining the precise schema for Spark DataFrames,\n","#                           ensuring type consistency and data quality.\n","# Spark data types: Ensures correct data interpretation and type consistency across the pipeline.\n","from pyspark.sql import SparkSession, DataFrame\n","from pyspark.sql.types import StructType, StructField, StringType, DoubleType, LongType, TimestampType\n","\n","logger.info(\"All necessary libraries have been imported successfully.\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":3,"statement_ids":[3],"state":"finished","livy_statement_state":"available","session_id":"db61d955-af2b-4104-b29b-c5b09f57c8af","normalized_state":"finished","queued_time":"2025-06-12T19:34:52.9348214Z","session_start_time":"2025-06-12T19:34:52.935841Z","execution_start_time":"2025-06-12T19:35:06.2972946Z","execution_finish_time":"2025-06-12T19:35:10.4854436Z","parent_msg_id":"83f44d07-0a71-41e7-b40f-8a5338a3c5b5"},"text/plain":"StatementMeta(, db61d955-af2b-4104-b29b-c5b09f57c8af, 3, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stderr","text":["2025-06-12 19:35:07,937 - INFO - All necessary libraries have been imported successfully.\n"]}],"execution_count":1,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"5d2df40f-6945-4c9f-8d47-e84d0cd28a45"},{"cell_type":"code","source":["# --- Cell 2: Data Schema Definition ---\n","\n","# Defines the explicit data schema for earthquake records obtained from the USGS API,\n","# along with additional metadata columns added during ingestion for auditing and lineage.\n","# This schema is critical for:\n","# 1. Ensuring strict type consistency when loading data into a Spark DataFrame.\n","# 2. Preventing schema inference issues that can lead to data quality problems in production.\n","# 3. Providing a clear contract for the data structure within the Lakehouse's Bronze layer.\n","earthquake_schema = StructType([\n","    StructField(\"id\", StringType(), True),           # Unique event ID provided by USGS.\n","    StructField(\"mag\", DoubleType(), True),          # Magnitude of the earthquake event.\n","    StructField(\"place\", StringType(), True),        # Geographical location description of the earthquake.\n","    StructField(\"time\", LongType(), True),           # Time of the event in milliseconds since the Unix epoch.\n","    StructField(\"updated\", LongType(), True),        # Time of the last update for the event (milliseconds epoch).\n","    StructField(\"tz\", StringType(), True),           # Timezone offset from UTC in minutes (historical field, often null).\n","    StructField(\"url\", StringType(), True),          # URL to the API for more event details.\n","    StructField(\"detail\", StringType(), True),       # URL for additional event details.\n","    StructField(\"felt\", LongType(), True),           # Number of reports from people who felt the earthquake.\n","    StructField(\"cdi\", DoubleType(), True),          # Community Decimal Intensity (CDI), a measure of shaking intensity.\n","    StructField(\"mmi\", DoubleType(), True),          # Modified Mercalli Intensity (MMI), a measure of shaking intensity.\n","    StructField(\"alert\", StringType(), True),        # USGS alert level (e.g., green, yellow, orange, red).\n","    StructField(\"status\", StringType(), True),       # Status of the event (e.g., automatic, reviewed, deleted).\n","    StructField(\"tsunami\", LongType(), True),        # Tsunami warning indicator (1 if warning issued, 0 if not).\n","    StructField(\"sig\", LongType(), True),            # Significance or impact of the event, a composite value.\n","    StructField(\"net\", StringType(), True),          # Seismic network that reported the event.\n","    StructField(\"code\", StringType(), True),         # Event identification code on the reporting network.\n","    StructField(\"ids\", StringType(), True),          # Comma-separated list of related event IDs.\n","    StructField(\"sources\", StringType(), True),      # Comma-separated list of data sources for the event.\n","    StructField(\"types\", StringType(), True),        # Comma-separated list of event types.\n","    StructField(\"nst\", LongType(), True),            # Number of monitoring stations used for event location.\n","    StructField(\"dmin\", DoubleType(), True),         # Horizontal distance from epicenter to the nearest station (degrees).\n","    StructField(\"rms\", DoubleType(), True),          # Root-mean-square of the travel time residuals (seconds).\n","    StructField(\"gap\", DoubleType(), True),          # Largest azimuthal gap in degrees between stations.\n","    StructField(\"magType\", StringType(), True),      # Type of magnitude reported (e.g., mb, ml, Mww).\n","    StructField(\"type\", StringType(), True),         # Primary classification of the event (e.g., earthquake, quarry blast).\n","    StructField(\"title\", StringType(), True),        # Descriptive title of the event.\n","    StructField(\"longitude\", DoubleType(), True),    # Longitude coordinate of the epicenter.\n","    StructField(\"latitude\", DoubleType(), True),     # Latitude coordinate of the epicenter.\n","    StructField(\"depth\", DoubleType(), True),        # Depth of the event in kilometers.\n","    StructField(\"ingestion_timestamp_utc\", TimestampType(), False) # Timestamp indicating when the record was ingested, always UTC.\n","])\n","\n","logger.info(\"'earthquake_schema' data schema defined and ready for use, including 'id' and 'ingestion_timestamp_utc'.\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":4,"statement_ids":[4],"state":"finished","livy_statement_state":"available","session_id":"db61d955-af2b-4104-b29b-c5b09f57c8af","normalized_state":"finished","queued_time":"2025-06-12T19:34:53.0175422Z","session_start_time":null,"execution_start_time":"2025-06-12T19:35:10.4879194Z","execution_finish_time":"2025-06-12T19:35:10.8028864Z","parent_msg_id":"a711fadf-d2eb-4051-8593-db7585521eb3"},"text/plain":"StatementMeta(, db61d955-af2b-4104-b29b-c5b09f57c8af, 4, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stderr","text":["2025-06-12 19:35:10,628 - INFO - 'earthquake_schema' data schema defined and ready for use, including 'id' and 'ingestion_timestamp_utc'.\n"]}],"execution_count":2,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"7fa9e2de-3be4-4d3c-87cb-bee6d0129d4e"},{"cell_type":"code","source":["# --- Cell 3: Configuration Parameters ---\n","\n","# This section defines all key parameters for the data ingestion process.\n","# Centralizing these values ensures easy modification and maintainability,\n","# supporting environmental configuration if needed in future iterations and\n","# promoting best practices for parameter management in data pipelines.\n","\n","# USGS API Endpoint:\n","# Base URL for querying earthquake data from the United States Geological Survey (USGS) API.\n","USGS_API_BASE_URL = \"https://earthquake.usgs.gov/fdsnws/event/1/query\"\n","\n","# Data Fetching Window:\n","# Specifies the number of days prior to the current ingestion timestamp\n","# for which earthquake data will be retrieved. A longer window fetches more historical data,\n","# useful for initial full loads or backfilling missing data.\n","DAYS_TO_FETCH = 365 # Configured to fetch a full year of historical data.\n","\n","# Minimum Magnitude Filter:\n","# Sets the lower threshold for earthquake magnitudes to be included in the ingestion.\n","# Adjusting this value directly impacts the volume and granularity of the ingested data.\n","# A value of 2.5 is chosen to ensure a sufficient dataset for analysis, including\n","# less significant, but still relevant, seismic events, while filtering out micro-earthquakes.\n","MIN_MAGNITUDE = 2.5\n","\n","# Ingestion Timestamp:\n","# Captures the exact UTC timestamp when this particular ingestion run initiated.\n","# Using UTC for consistency across different environments, time zones, and for\n","# robust data lineage. This value is crucial for auditing, tracking data freshness,\n","# and can be used for partitioning raw data in the Lakehouse (e.g., year, month, day of ingestion).\n","INGESTION_DATETIME = datetime.now(timezone.utc) # Explicitly set to UTC for universal consistency.\n","\n","# Lakehouse Bronze Layer Target:\n","# Defines the fully qualified name of the table in the Bronze layer of the Lakehouse\n","# where the raw, untransformed data will be stored. This serves as the immutable\n","# landing zone for all ingested earthquake records, providing a reliable source\n","# for subsequent transformations.\n","BRONZE_TABLE_NAME = \"bronze_usgs_earthquakes\"\n","\n","# Log the configured parameters for traceability and debugging in production environments.\n","# This helps in understanding the exact parameters used for a specific run.\n","logger.info(\"Ingestion Configuration Loaded:\")\n","logger.info(f\"  USGS API Base URL: {USGS_API_BASE_URL}\")\n","logger.info(f\"  Data Fetching Window: Last {DAYS_TO_FETCH} days\")\n","logger.info(f\"  Minimum Magnitude: {MIN_MAGNITUDE}\")\n","logger.info(f\"  Ingestion Datetime (UTC): {INGESTION_DATETIME.isoformat()}\") # Using ISO format for clear timestamp.\n","logger.info(f\"  Bronze Target Table: {BRONZE_TABLE_NAME}\")\n","\n","# Spark Session Initialization:\n","\n","try:\n","    # In Azure Fabric notebooks, the 'spark' session is typically pre-initialized and available globally.\n","    # However, using `SparkSession.builder.getOrCreate()` is a robust pattern as it\n","    # either retrieves the existing session or creates a new one if necessary, making the script\n","    # more portable across different Spark environments.\n","    spark = SparkSession.builder \\\n","                        .appName(\"USGSEarthquakeIngestion\") \\\n","                        .getOrCreate()\n","    logger.info(\"Spark Session initialized or retrieved successfully.\")\n","except Exception as e:\n","    logger.error(f\"Error initializing or retrieving Spark Session: {e}\")\n","    raise # Re-raise the exception to halt execution if Spark cannot be started."],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":5,"statement_ids":[5],"state":"finished","livy_statement_state":"available","session_id":"db61d955-af2b-4104-b29b-c5b09f57c8af","normalized_state":"finished","queued_time":"2025-06-12T19:34:53.0769192Z","session_start_time":null,"execution_start_time":"2025-06-12T19:35:10.8050685Z","execution_finish_time":"2025-06-12T19:35:11.0993958Z","parent_msg_id":"0b39f3cf-fc9a-435c-b23e-2a3491a437b1"},"text/plain":"StatementMeta(, db61d955-af2b-4104-b29b-c5b09f57c8af, 5, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stderr","text":["2025-06-12 19:35:10,907 - INFO - Ingestion Configuration Loaded:\n2025-06-12 19:35:10,908 - INFO -   USGS API Base URL: https://earthquake.usgs.gov/fdsnws/event/1/query\n2025-06-12 19:35:10,908 - INFO -   Data Fetching Window: Last 365 days\n2025-06-12 19:35:10,909 - INFO -   Minimum Magnitude: 2.5\n2025-06-12 19:35:10,910 - INFO -   Ingestion Datetime (UTC): 2025-06-12T19:35:10.907037+00:00\n2025-06-12 19:35:10,911 - INFO -   Bronze Target Table: bronze_usgs_earthquakes\n"]}],"execution_count":3,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"a478d3a3-11d8-48d7-bc1a-4bcf77d1bbbb"},{"cell_type":"code","source":["# --- Cell 4: Function to Extract Earthquake Data ---\n","\n","def fetch_earthquake_data(start_time: datetime, end_time: datetime, min_magnitude: float = 2.5, limit: int = 20000) -> pd.DataFrame:\n","    \"\"\"\n","    Fetches earthquake data from the USGS API for a specified time range and minimum magnitude.\n","    This function handles API pagination to retrieve all available records up to the configured limit\n","    per request, iteratively querying the API until all data is retrieved or the API signals no more.\n","\n","    The USGS API has a typical limit of 20,000 events per request. This function\n","    iteratively fetches data by incrementing the 'offset' parameter until all\n","    records for the specified time range are retrieved or no more data is available\n","    (e.g., when the number of returned features is less than the requested limit).\n","\n","    Args:\n","        start_time (datetime): The start datetime for the data query (inclusive, UTC).\n","        end_time (datetime): The end datetime for the data query (inclusive, UTC).\n","        min_magnitude (float): The minimum magnitude of earthquakes to fetch. Defaults to 2.5.\n","        limit (int): The maximum number of events to fetch per single API request. Defaults to 20000.\n","\n","    Returns:\n","        pd.DataFrame: A Pandas DataFrame containing the extracted earthquake records,\n","                      structured according to the defined schema.\n","                      Returns an empty DataFrame if no data is found or an error occurs\n","                      during the API interaction or data processing.\n","    \"\"\"\n","    # Parameters for the USGS API request.\n","    # Datetime objects are formatted to ISO 8601 string (`%Y-%m-%dT%H:%M:%S`) as required by the API.\n","    params = {\n","        'format': 'geojson',             # Request data in GeoJSON format.\n","        'starttime': start_time.strftime('%Y-%m-%dT%H:%M:%S'),\n","        'endtime': end_time.strftime('%Y-%m-%dT%H:%M:%S'),\n","        'minmagnitude': min_magnitude,   # Filter by minimum earthquake magnitude.\n","        'limit': limit,                  # Number of results per page/request.\n","        'offset': 1                      # USGS API offset is 1-based for pagination.\n","    }\n","    \n","    all_features = [] # List to accumulate all earthquake features across multiple API calls.\n","    \n","    logger.info(f\"Initiating data fetch from USGS API for time range: {start_time.isoformat()} to {end_time.isoformat()} with min magnitude {min_magnitude}.\")\n","    \n","    # Loop for pagination: Continuously fetch data until no more records are available\n","    # or the last page has been retrieved based on the 'limit' parameter.\n","    request_count = 0\n","    while True:\n","        request_count += 1\n","        logger.info(f\"Making API request {request_count} with offset: {params['offset']}\")\n","        try:\n","            # Execute HTTP GET request to the USGS API.\n","            # A timeout is critical for robust production systems to prevent indefinite waits\n","            # and ensure the script doesn't hang on unresponsive API endpoints.\n","            response = requests.get(USGS_API_BASE_URL, params=params, timeout=120) # Increased timeout to 120s for large responses.\n","            response.raise_for_status()  # Raise an HTTPError for bad responses (4xx or 5xx status codes).\n","\n","            # Parse the JSON response body.\n","            data = response.json()\n","            \n","            # Extract the 'features' array, which contains the individual earthquake event details.\n","            features = data.get('features', [])\n","            all_features.extend(features) # Add newly fetched features to the collective list.\n","            logger.info(f\"Received {len(features)} features in current API response. Total features collected: {len(all_features)}.\")\n","            \n","            # Pagination check:\n","            # The loop breaks if the number of features received in the current response is less than\n","            # the requested limit. This typically indicates that the last page of data has been fetched,\n","            # as there are no more records to fill a full 'limit' page.\n","            if len(features) < params['limit']:\n","                logger.info(\"Reached end of data or received fewer features than limit, stopping pagination.\")\n","                break # No more data to fetch or last page reached.\n","            \n","            # Increment the offset for the next request to fetch the subsequent page of data.\n","            params['offset'] += params['limit']\n","            \n","        # Comprehensive error handling for various request issues ensures robustness.\n","        except requests.exceptions.Timeout as e:\n","            logger.error(f\"API request timed out after {response.request.timeout} seconds: {e}\")\n","            return pd.DataFrame() # Return empty DataFrame to signal failure without crashing the pipeline.\n","        except requests.exceptions.RequestException as e:\n","            logger.error(f\"Error fetching data from USGS API (RequestException - e.g., network error, bad URL): {e}\")\n","            return pd.DataFrame() # Return empty DataFrame on HTTP/network errors.\n","        except json.JSONDecodeError as e:\n","            logger.error(f\"Error decoding JSON response from USGS API: {e}. Response content (first 500 chars): {response.text[:500]}...\")\n","            return pd.DataFrame() # Return empty DataFrame on invalid JSON.\n","        except Exception as e:\n","            logger.error(f\"An unexpected error occurred during API fetch: {e}\", exc_info=True) # exc_info=True logs the full traceback.\n","            return pd.DataFrame()\n","\n","    if not all_features:\n","        logger.warning(\"No earthquake features found for the specified criteria. Returning empty DataFrame.\")\n","        return pd.DataFrame()\n","\n","    # --- Data Transformation to Pandas DataFrame ---\n","    # Iterates through each extracted GeoJSON feature and constructs a dictionary\n","    # mapping API fields to the desired column names (matching `earthquake_schema`).\n","    # This ensures consistency for downstream processing and storage in the Lakehouse.\n","    earthquakes_list = []\n","    for feature in all_features:\n","        properties = feature.get('properties', {}) # Extract event properties.\n","        geometry = feature.get('geometry', {})     # Extract geometry information (coordinates).\n","        # Coordinates are typically an array: [longitude, latitude, depth_km].\n","        coordinates = geometry.get('coordinates', [None, None, None]) \n","        \n","        earthquake_record = {\n","            'id': feature.get('id'),                # Directly from the top-level feature object.\n","            'mag': properties.get('mag'),\n","            'place': properties.get('place'),\n","            'time': properties.get('time'),\n","            'updated': properties.get('updated'),\n","            'tz': properties.get('tz'),\n","            'url': properties.get('url'),\n","            'detail': properties.get('detail'),\n","            'felt': properties.get('felt'),\n","            'cdi': properties.get('cdi'),\n","            'mmi': properties.get('mmi'),\n","            'alert': properties.get('alert'),\n","            'status': properties.get('status'),\n","            'tsunami': properties.get('tsunami', 0), # Default to 0 if not present to ensure a value.\n","            'sig': properties.get('sig'),\n","            'net': properties.get('net'),\n","            'code': properties.get('code'),\n","            'ids': properties.get('ids'),\n","            'sources': properties.get('sources'),\n","            'types': properties.get('types'),\n","            'nst': properties.get('nst'),\n","            'dmin': properties.get('dmin'),\n","            'rms': properties.get('rms'),\n","            'gap': properties.get('gap'),\n","            'magType': properties.get('magType'),\n","            'type': properties.get('type'),\n","            'title': properties.get('title'),\n","            'longitude': coordinates[0] if len(coordinates) > 0 else None, # Safely access coordinate elements.\n","            'latitude': coordinates[1] if len(coordinates) > 1 else None,\n","            'depth': coordinates[2] if len(coordinates) > 2 else None,\n","            'ingestion_timestamp_utc': INGESTION_DATETIME # Add ingestion timestamp for data lineage and auditing.\n","        }\n","        earthquakes_list.append(earthquake_record)\n","    \n","    # Create Pandas DataFrame from the list of dictionaries.\n","    # This DataFrame will be subsequently converted to a Spark DataFrame.\n","    df = pd.DataFrame(earthquakes_list)\n","    logger.info(f\"Successfully extracted and prepared {len(df)} earthquake records as a Pandas DataFrame.\")\n","    return df"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":6,"statement_ids":[6],"state":"finished","livy_statement_state":"available","session_id":"db61d955-af2b-4104-b29b-c5b09f57c8af","normalized_state":"finished","queued_time":"2025-06-12T19:34:53.1957807Z","session_start_time":null,"execution_start_time":"2025-06-12T19:35:11.101843Z","execution_finish_time":"2025-06-12T19:35:11.4337123Z","parent_msg_id":"9e62ad53-3dc0-43b5-8481-30fb79a2cdc9"},"text/plain":"StatementMeta(, db61d955-af2b-4104-b29b-c5b09f57c8af, 6, Finished, Available, Finished)"},"metadata":{}}],"execution_count":4,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"fe5b83f0-374e-48b3-ad91-e81a19b69f72"},{"cell_type":"code","source":["# --- Cell 5: Execute Data Extraction ---\n","\n","# This cell orchestrates the data extraction by defining the precise time window\n","# for the API query and calling the `fetch_earthquake_data` function.\n","# It serves as the main execution point for the data retrieval logic.\n","\n","# Determine the end time for data retrieval. Using `datetime.now(timezone.utc)`\n","# ensures consistency with the `INGESTION_DATETIME` and avoids timezone-related issues,\n","# critical for accurate historical data queries.\n","end_datetime_utc = datetime.now(timezone.utc)\n","\n","# Calculate the start time based on the configured `DAYS_TO_FETCH` and the `end_datetime_utc`.\n","# This defines the historical window for which data will be ingested.\n","start_datetime_utc = end_datetime_utc - timedelta(days=DAYS_TO_FETCH)\n","\n","# Log the exact time range for which data will be fetched. This is useful for auditing,\n","# understanding the scope of the current ingestion run, and debugging.\n","logger.info(f\"Initiating earthquake data fetch for the period: {start_datetime_utc.isoformat()} to {end_datetime_utc.isoformat()} (UTC).\")\n","\n","# Call the data fetching function with the defined parameters.\n","# The result is stored in a Pandas DataFrame (`df_earthquakes_pd`), which will then be\n","# transformed into a Spark DataFrame in the next step.\n","df_earthquakes_pd = fetch_earthquake_data(start_datetime_utc, end_datetime_utc, MIN_MAGNITUDE)\n","\n","# Validate the outcome of the data extraction.\n","if not df_earthquakes_pd.empty:\n","    # Log the successful retrieval and show a sample of the data.\n","    # Displaying the head() provides a quick visual check of the data structure and content.\n","    logger.info(f\"Successfully fetched {len(df_earthquakes_pd)} earthquake records.\")\n","    logger.info(f\"First 5 records of the fetched data:\\n{df_earthquakes_pd.head().to_string()}\")\n","else:\n","    # Log a warning if no data was fetched. This indicates a potential issue\n","    # with the API, specified parameters, or simply a lack of data for the period.\n","    logger.warning(\"No earthquake data was fetched for the specified criteria. The DataFrame is empty.\")\n","    # In a production pipeline, depending on criticality, an empty DataFrame might:\n","    # 1. Trigger an alert or fail the job if data presence is a strict requirement.\n","    # 2. Be handled gracefully by downstream processes (as is done here)."],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":7,"statement_ids":[7],"state":"finished","livy_statement_state":"available","session_id":"db61d955-af2b-4104-b29b-c5b09f57c8af","normalized_state":"finished","queued_time":"2025-06-12T19:34:53.2434885Z","session_start_time":null,"execution_start_time":"2025-06-12T19:35:11.4360227Z","execution_finish_time":"2025-06-12T19:35:21.1212252Z","parent_msg_id":"bf0901cb-1968-481e-a4f0-37a110fdf1ae"},"text/plain":"StatementMeta(, db61d955-af2b-4104-b29b-c5b09f57c8af, 7, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stderr","text":["2025-06-12 19:35:11,536 - INFO - Initiating earthquake data fetch for the period: 2024-06-12T19:35:11.536063+00:00 to 2025-06-12T19:35:11.536063+00:00 (UTC).\n2025-06-12 19:35:11,537 - INFO - Initiating data fetch from USGS API for time range: 2024-06-12T19:35:11.536063+00:00 to 2025-06-12T19:35:11.536063+00:00 with min magnitude 2.5.\n2025-06-12 19:35:11,537 - INFO - Making API request 1 with offset: 1\n2025-06-12 19:35:15,737 - INFO - Received 20000 features in current API response. Total features collected: 20000.\n2025-06-12 19:35:15,738 - INFO - Making API request 2 with offset: 20001\n2025-06-12 19:35:19,408 - INFO - Received 4588 features in current API response. Total features collected: 24588.\n2025-06-12 19:35:19,409 - INFO - Reached end of data or received fewer features than limit, stopping pagination.\n2025-06-12 19:35:19,609 - INFO - Successfully extracted and prepared 24588 earthquake records as a Pandas DataFrame.\n2025-06-12 19:35:19,639 - INFO - Successfully fetched 24588 earthquake records.\n2025-06-12 19:35:19,763 - INFO - First 5 records of the fetched data:\n             id   mag                                 place           time        updated    tz                                                             url                                                                                detail  felt  cdi  mmi alert     status  tsunami  sig net        code                        ids  sources                                                           types   nst     dmin   rms    gap magType        type                                         title   longitude   latitude     depth          ingestion_timestamp_utc\n0    ci40992455  2.83         16 km WSW of Johannesburg, CA  1749754825090  1749755457960  None    https://earthquake.usgs.gov/earthquakes/eventpage/ci40992455    https://earthquake.usgs.gov/fdsnws/event/1/query?eventid=ci40992455&format=geojson   NaN  NaN  NaN  None  automatic        0  123  ci    40992455               ,ci40992455,     ,ci,  ,focal-mechanism,nearby-cities,origin,phase-data,scitech-link,  60.0  0.07195  0.20   36.0      ml  earthquake         M 2.8 - 16 km WSW of Johannesburg, CA -117.807500  35.332333    7.2800 2025-06-12 19:35:10.907037+00:00\n1    us6000qjwg  4.10            9 km SE of Kamaishi, Japan  1749751347586  1749753280040  None    https://earthquake.usgs.gov/earthquakes/eventpage/us6000qjwg    https://earthquake.usgs.gov/fdsnws/event/1/query?eventid=us6000qjwg&format=geojson   NaN  NaN  NaN  None   reviewed        0  259  us    6000qjwg               ,us6000qjwg,     ,us,                                             ,origin,phase-data,  27.0  1.70600  0.25  193.0      mb  earthquake            M 4.1 - 9 km SE of Kamaishi, Japan  141.960800  39.226100   52.6800 2025-06-12 19:35:10.907037+00:00\n2    ci40992391  2.63                13 km WSW of Heber, CA  1749751268560  1749751922300  None    https://earthquake.usgs.gov/earthquakes/eventpage/ci40992391    https://earthquake.usgs.gov/fdsnws/event/1/query?eventid=ci40992391&format=geojson   NaN  NaN  NaN  None  automatic        0  106  ci    40992391               ,ci40992391,     ,ci,  ,focal-mechanism,nearby-cities,origin,phase-data,scitech-link,  48.0  0.07046  0.31   48.0      ml  earthquake                M 2.6 - 13 km WSW of Heber, CA -115.661333  32.693667   12.9600 2025-06-12 19:35:10.907037+00:00\n3  tx2025lmsmdc  2.90    57 km S of Whites City, New Mexico  1749749452441  1749750411040  None  https://earthquake.usgs.gov/earthquakes/eventpage/tx2025lmsmdc  https://earthquake.usgs.gov/fdsnws/event/1/query?eventid=tx2025lmsmdc&format=geojson   NaN  NaN  NaN  None   reviewed        0  129  tx  2025lmsmdc  ,us6000qjv9,tx2025lmsmdc,  ,us,tx,                                             ,origin,phase-data,  38.0  0.00000  0.20   67.0      ml  earthquake    M 2.9 - 57 km S of Whites City, New Mexico -104.294000  31.663000    7.6379 2025-06-12 19:35:10.907037+00:00\n4    us6000qjv7  4.80  114 km NE of Kimbe, Papua New Guinea  1749749129641  1749750192040  None    https://earthquake.usgs.gov/earthquakes/eventpage/us6000qjv7    https://earthquake.usgs.gov/fdsnws/event/1/query?eventid=us6000qjv7&format=geojson   NaN  NaN  NaN  None   reviewed        0  354  us    6000qjv7               ,us6000qjv7,     ,us,                                             ,origin,phase-data,  77.0  1.41900  0.72   54.0      mb  earthquake  M 4.8 - 114 km NE of Kimbe, Papua New Guinea  150.900100  -4.848500  253.1620 2025-06-12 19:35:10.907037+00:00\n"]}],"execution_count":5,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4507812e-eecb-4c5e-919b-3f3b61cbcece"},{"cell_type":"code","source":["#--- Cell 6: Convert Pandas DataFrame to Spark DataFrame and Save to Bronze Layer ---\n","\n","# This cell performs the critical step of transforming the Pandas DataFrame\n","# containing the extracted data into a Spark DataFrame. It then persists this\n","# Spark DataFrame to the Bronze layer of the Lakehouse, which serves as the\n","# raw, immutable landing zone for all ingested data, providing a single source\n","# of truth for raw seismic information.\n","\n","if not df_earthquakes_pd.empty:\n","    logger.info(f\"Converting Pandas DataFrame with {len(df_earthquakes_pd)} records to Spark DataFrame.\")\n","\n","    try:\n","        # In Azure Fabric notebooks, the 'spark' session is typically pre-initialized and available globally.\n","        # This check verifies its availability and attempts to get or create one if not found,\n","        # ensuring the script can run in various Spark environments.\n","        if 'spark' not in globals() or not isinstance(spark, SparkSession):\n","            logger.error(\"SparkSession 'spark' is not initialized. Attempting to get or create a new one.\")\n","            spark = SparkSession.builder.appName(\"USGSEarthquakeIngestion\").getOrCreate()\n","        \n","        # Create the Spark DataFrame using the explicitly defined schema (`earthquake_schema`).\n","        # This is a critical step for production environments as it ensures data types\n","        # are correctly interpreted and maintained, preventing potential schema inference issues\n","        # that could lead to data corruption or pipeline failures downstream.\n","        df_earthquakes_spark = spark.createDataFrame(df_earthquakes_pd, schema=earthquake_schema)\n","        logger.info(f\"Successfully converted Pandas DataFrame to Spark DataFrame with {df_earthquakes_spark.count()} records.\")\n","        logger.info(\"Spark DataFrame Schema:\")\n","        df_earthquakes_spark.printSchema() # Log the Spark DataFrame schema for verification.\n","\n","        # Write the Spark DataFrame to a Delta table in the Bronze layer of the Lakehouse.\n","        # This leverages Delta Lake's capabilities for robust data storage.\n","        # `format(\"delta\")`: Specifies Delta Lake format for ACID properties, schema enforcement, etc.\n","        # `mode(\"overwrite\")`: Replaces the entire table with the new data. This is suitable for\n","        #                      initial full loads or when the entire dataset is refreshed.\n","        #                      For incremental updates, `append` or `merge` modes are typically used.\n","        # `option(\"overwriteSchema\", \"true\")`: Allows the target Delta table's schema to adapt\n","        #                                       to the incoming data's schema. While useful for\n","        #                                       Bronze to capture source schema evolution, use with\n","        #                                       caution in Silver/Gold layers where schema stability\n","        #                                       is more critical.\n","        # `saveAsTable(BRONZE_TABLE_NAME)`: Registers the data as a named table within the Lakehouse\n","        #                                    metadata, making it easily queryable via SQL.\n","        logger.info(f\"Writing {df_earthquakes_spark.count()} records to Bronze table: {BRONZE_TABLE_NAME} using 'overwrite' mode.\")\n","        df_earthquakes_spark.write \\\n","                            .format(\"delta\") \\\n","                            .mode(\"overwrite\") \\\n","                            .option(\"overwriteSchema\", \"true\") \\\n","                            .saveAsTable(BRONZE_TABLE_NAME)\n","        \n","        logger.info(f\"Data successfully persisted to Bronze table: {BRONZE_TABLE_NAME}.\")\n","        \n","\n","    except Exception as e:\n","        logger.error(f\"An error occurred during Spark DataFrame conversion or saving to Bronze layer: {e}\", exc_info=True)\n","else:\n","    logger.warning(\"Skipping Spark DataFrame processing and saving to Bronze layer as no data was fetched in the previous step.\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":8,"statement_ids":[8],"state":"finished","livy_statement_state":"available","session_id":"db61d955-af2b-4104-b29b-c5b09f57c8af","normalized_state":"finished","queued_time":"2025-06-12T19:34:53.3051187Z","session_start_time":null,"execution_start_time":"2025-06-12T19:35:21.1234472Z","execution_finish_time":"2025-06-12T19:35:56.8969504Z","parent_msg_id":"c5b29898-7506-4acc-a073-fd8b6cf4f0f8"},"text/plain":"StatementMeta(, db61d955-af2b-4104-b29b-c5b09f57c8af, 8, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stderr","text":["2025-06-12 19:35:21,254 - INFO - Converting Pandas DataFrame with 24588 records to Spark DataFrame.\n2025-06-12 19:35:23,992 - INFO - Writing 24588 records to Bronze table: bronze_usgs_earthquakes using 'overwrite' mode.\n2025-06-12 19:35:54,636 - INFO - Data successfully persisted to Bronze table: bronze_usgs_earthquakes.\n"]}],"execution_count":6,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"a5ac9461-17a8-45f2-bdd2-c50fc8b5da6c"},{"cell_type":"code","source":["# --- Cell 7: Display Sample from Bronze Table (Optional Verification) ---\n","\n","# This cell performs a quick verification by querying the newly created\n","# or updated Bronze Delta table and displaying a sample of its contents.\n","# This helps confirm that the data was written correctly, is accessible\n","# within the Lakehouse environment, and that the schema has been applied as expected.\n","# This step is optional but highly recommended for development and initial deployment.\n","\n","if not df_earthquakes_pd.empty: # Only proceed with verification if data was actually ingested.\n","    try:\n","        # Ensure SparkSession is available to query the table.\n","        # This check is crucial if this cell were to be run in isolation or\n","        # after a Spark session might have expired or been reset.\n","        if 'spark' not in globals() or not isinstance(spark, SparkSession):\n","            logger.error(\"SparkSession 'spark' is not initialized for table verification. Attempting to get or create one.\")\n","            spark = SparkSession.builder.appName(\"USGSEarthquakeVerification\").getOrCreate()\n","            \n","        logger.info(f\"Displaying a sample of 5 records from the Bronze table '{BRONZE_TABLE_NAME}' for verification.\")\n","        # Retrieve the table as a Spark DataFrame using `spark.table()` and show its first 5 rows.\n","        # `truncate=False` ensures that column values are not truncated in the output,\n","        # providing a full view of the data for verification.\n","        spark.table(BRONZE_TABLE_NAME).show(5, truncate=False)\n","        \n","        # Log the schema and total count for further verification.\n","        # This provides a programmatic confirmation of the table's structure and size.\n","        logger.info(f\"Bronze table '{BRONZE_TABLE_NAME}' schema:\")\n","        spark.table(BRONZE_TABLE_NAME).printSchema()\n","        logger.info(f\"Total records in Bronze table '{BRONZE_TABLE_NAME}': {spark.table(BRONZE_TABLE_NAME).count()}\")\n","\n","    except Exception as e:\n","        logger.error(f\"An error occurred while trying to read and display data from Bronze table '{BRONZE_TABLE_NAME}': {e}\", exc_info=True)\n","else:\n","    logger.info(\"Skipping Bronze table display as no data was processed and written to the table in prior steps.\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":9,"statement_ids":[9],"state":"finished","livy_statement_state":"available","session_id":"db61d955-af2b-4104-b29b-c5b09f57c8af","normalized_state":"finished","queued_time":"2025-06-12T19:34:53.3531358Z","session_start_time":null,"execution_start_time":"2025-06-12T19:35:56.8994496Z","execution_finish_time":"2025-06-12T19:36:03.0714854Z","parent_msg_id":"2541c4c8-2f18-4f94-b57b-93e564f0a151"},"text/plain":"StatementMeta(, db61d955-af2b-4104-b29b-c5b09f57c8af, 9, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stderr","text":["2025-06-12 19:35:56,992 - INFO - Displaying a sample of 5 records from the Bronze table 'bronze_usgs_earthquakes' for verification.\n2025-06-12 19:36:02,562 - INFO - Total records in Bronze table 'bronze_usgs_earthquakes': 24588\n"]}],"execution_count":7,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"62bdc52c-ea90-43c5-94d5-8d18a0b61ef8"},{"cell_type":"code","source":[],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"2cdca1b0-044c-4bd6-b322-f5abd3ba8aec"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"default_lakehouse":"52dbc9c4-a18f-4d2e-b0fb-6e139836330a","known_lakehouses":[{"id":"52dbc9c4-a18f-4d2e-b0fb-6e139836330a"}],"default_lakehouse_name":"EarthquakeDataLakehouse","default_lakehouse_workspace_id":"ac391401-c034-4101-92c2-b9d4de1dc29a"}}},"nbformat":4,"nbformat_minor":5}