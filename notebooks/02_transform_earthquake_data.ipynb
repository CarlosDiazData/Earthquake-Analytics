{"cells":[{"cell_type":"code","source":["# --- Cell 1: Notebook Header, Logging Configuration, and Library Imports ---\n","\n","\"\"\"\n","Notebook: 02_transform_earthquake_data.ipynb\n","\n","Purpose:\n","This notebook is responsible for cleaning, transforming, and enriching raw earthquake data\n","from the Bronze layer, and persisting the refined data into the Silver layer of the Lakehouse.\n","The Silver layer typically contains enterprise-wide, clean, conformed, and semi-enriched data,\n","ready for direct reporting, advanced analytics, and further specialized enrichment in the Gold layer.\n","\n","Dependencies:\n","- Python 3.x\n","- pyspark library (for distributed processing and DataFrame operations)\n","\n","Execution Environment:\n","This script is designed to run within an Apache Spark environment,\n","specifically optimized for platforms like Azure Fabric where a SparkSession\n","('spark') is typically pre-initialized and available globally.\n","\"\"\"\n","\n","# Configure a basic logging system for effective monitoring in production environments.\n","# This setup allows capturing informational messages, warnings, and errors throughout\n","# the script's execution, which is crucial for debugging and operational oversight.\n","import logging\n","# Ensure the logger is configured only once if this cell might be run multiple times in a session.\n","# Basic configuration sets up a handler that prints log messages to the console, which Fabric captures.\n","logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n","logger = logging.getLogger(__name__)\n","\n","# Standard PySpark libraries are imported for DataFrame operations and built-in functions.\n","from pyspark.sql import SparkSession            # The entry point for Spark functionality.\n","from pyspark.sql import functions as F          # Provides access to Spark SQL functions (e.g., F.col, F.year).\n","from pyspark.sql.types import TimestampType, IntegerType, DoubleType, StringType, BooleanType # Specific Spark data types for schema definition and casting.\n","\n","logger.info(\"All necessary libraries have been imported successfully for data transformation.\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":3,"statement_ids":[3],"state":"finished","livy_statement_state":"available","session_id":"f21af01b-40ba-416d-93ca-65d214b6f00e","normalized_state":"finished","queued_time":"2025-06-12T19:38:49.6377819Z","session_start_time":"2025-06-12T19:38:49.6389762Z","execution_start_time":"2025-06-12T19:39:01.7773738Z","execution_finish_time":"2025-06-12T19:39:02.2056748Z","parent_msg_id":"90ea85dd-f3e2-4b4b-9b10-cca2012f77d8"},"text/plain":"StatementMeta(, f21af01b-40ba-416d-93ca-65d214b6f00e, 3, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stderr","text":["2025-06-12 19:39:01,902 - INFO - All necessary libraries have been imported successfully for data transformation.\n"]}],"execution_count":1,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"9b6dcacf-4961-4d93-8a06-bd406ed9ecb2"},{"cell_type":"code","source":["# --- Cell 2: Configuration Parameters ---\n","\n","# This section defines all key parameters for the data transformation process.\n","# Centralizing these values ensures easy modification and maintainability,\n","# supporting environmental configuration and promoting best practices for parameter management.\n","\n","# Source Table Name (Bronze Layer):\n","# The fully qualified name of the Delta table in the Bronze layer from which raw earthquake data\n","# will be read. This is the raw, immutable landing zone from the ingestion step.\n","BRONZE_TABLE_NAME = \"bronze_usgs_earthquakes\"\n","\n","# Target Table Name (Silver Layer):\n","# The fully qualified name of the Delta table in the Silver layer where cleaned, transformed,\n","# and partially enriched data will be stored. This layer is conformed and ready for consumption.\n","SILVER_TABLE_NAME = \"silver_earthquakes_cleaned\"\n","\n","# Base File Path for Silver Layer (Optional for file-based storage):\n","# This defines the base path for storing data as files, often in Delta Lake format.\n","# While the primary target is a managed Delta table, saving to files can be used for\n","# archiving, cross-platform compatibility, or specific downstream tools that prefer direct file access.\n","SILVER_FILE_PATH_BASE = \"Files/silver/earthquakes_cleaned\"\n","\n","# Processing Timestamp:\n","# Captures the exact UTC timestamp when this particular transformation run initiated.\n","# Using `F.current_timestamp()` ensures this timestamp is generated consistently\n","# within the Spark execution environment and is automatically aligned with Spark's time context.\n","# This timestamp is crucial for auditing, data lineage, and can be used for incremental loading strategies.\n","PROCESSING_TIMESTAMP_UTC = F.current_timestamp()\n","\n","logger.info(\"Transformation Configuration Loaded:\")\n","logger.info(f\"  Reading from Bronze table: {BRONZE_TABLE_NAME}\")\n","logger.info(f\"  Writing to Silver table: {SILVER_TABLE_NAME}\")\n","logger.info(f\"  Processing Timestamp (captured during Spark execution): {PROCESSING_TIMESTAMP_UTC}\") # Note: This will be a Spark Literal for internal use."],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":4,"statement_ids":[4],"state":"finished","livy_statement_state":"available","session_id":"f21af01b-40ba-416d-93ca-65d214b6f00e","normalized_state":"finished","queued_time":"2025-06-12T19:38:49.6792569Z","session_start_time":null,"execution_start_time":"2025-06-12T19:39:02.2080321Z","execution_finish_time":"2025-06-12T19:39:02.570301Z","parent_msg_id":"a9864caf-5a74-43c8-8aa4-981767b46244"},"text/plain":"StatementMeta(, f21af01b-40ba-416d-93ca-65d214b6f00e, 4, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stderr","text":["2025-06-12 19:39:02,341 - INFO - Transformation Configuration Loaded:\n2025-06-12 19:39:02,341 - INFO -   Reading from Bronze table: bronze_usgs_earthquakes\n2025-06-12 19:39:02,342 - INFO -   Writing to Silver table: silver_earthquakes_cleaned\n2025-06-12 19:39:02,344 - INFO -   Processing Timestamp (captured during Spark execution): Column<'current_timestamp()'>\n"]}],"execution_count":2,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"bd2e67fe-a422-4659-bc96-fc7225741858"},{"cell_type":"code","source":["# --- Cell 3: Initialize Spark Session and Load Bronze Data ---\n","\n","# This cell is responsible for initializing the Spark session, which is the entry point\n","# for all Spark functionality, and then loading the raw earthquake data from the Bronze layer.\n","\n","try:\n","    # In Azure Fabric notebooks, the 'spark' session is typically pre-initialized and available globally.\n","    # However, using `SparkSession.builder.getOrCreate()` is a robust pattern as it\n","    # either retrieves the existing session or creates a new one if necessary, making the script\n","    # more portable across different Spark environments.\n","    if 'spark' not in globals() or not isinstance(spark, SparkSession):\n","        logger.info(\"SparkSession 'spark' not found or not an instance of SparkSession. Attempting to get or create a new one.\")\n","        spark = SparkSession.builder \\\n","                            .appName(\"EarthquakeDataTransformation\") \\\n","                            .getOrCreate()\n","        logger.info(\"Spark Session initialized or retrieved successfully.\")\n","    else:\n","        logger.info(\"Spark Session 'spark' is already initialized and available.\")\n","\n","    # Load data from the Bronze layer Delta table into a Spark DataFrame.\n","    # The Bronze layer is expected to contain raw, untransformed data as ingested from the source.\n","    df_bronze = spark.table(BRONZE_TABLE_NAME)\n","    \n","    # Log the number of records read and display the schema for immediate verification.\n","    # This helps confirm that the correct data has been loaded and its structure is as expected.\n","    logger.info(f\"Successfully read {df_bronze.count()} records from Bronze table: {BRONZE_TABLE_NAME}.\")\n","    logger.info(\"Bronze DataFrame Schema:\")\n","    df_bronze.printSchema()\n","\n","    # Check if the Bronze table is empty.\n","    # If the input data is empty, subsequent transformation steps might result in an empty Silver layer.\n","    if df_bronze.count() == 0:\n","        logger.warning(f\"Bronze table '{BRONZE_TABLE_NAME}' is empty. No data to transform.\")\n","\n","\n","except Exception as e:\n","    # Critical error handling: If the Bronze table cannot be loaded, the transformation cannot proceed.\n","    logger.error(f\"FATAL ERROR: Failed to read Bronze table '{BRONZE_TABLE_NAME}'. Transformation cannot proceed. Error: {e}\", exc_info=True)\n","    # Raising an exception here will cause the pipeline activity to fail, triggering alerts\n","    # in an orchestrated environment, which is the desired behavior for unrecoverable errors.\n","    raise Exception(f\"Transformation failed due to an error loading Bronze data: {e}\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":5,"statement_ids":[5],"state":"finished","livy_statement_state":"available","session_id":"f21af01b-40ba-416d-93ca-65d214b6f00e","normalized_state":"finished","queued_time":"2025-06-12T19:38:49.8034266Z","session_start_time":null,"execution_start_time":"2025-06-12T19:39:02.5725621Z","execution_finish_time":"2025-06-12T19:39:17.9912191Z","parent_msg_id":"f0f54030-950f-4526-b93f-62c0d5a2cd25"},"text/plain":"StatementMeta(, f21af01b-40ba-416d-93ca-65d214b6f00e, 5, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stderr","text":["2025-06-12 19:39:02,682 - INFO - Spark Session 'spark' is already initialized and available.\n"]},{"output_type":"stream","name":"stdout","text":["root\n |-- id: string (nullable = true)\n |-- mag: double (nullable = true)\n |-- place: string (nullable = true)\n |-- time: long (nullable = true)\n |-- updated: long (nullable = true)\n |-- tz: string (nullable = true)\n |-- url: string (nullable = true)\n |-- detail: string (nullable = true)\n |-- felt: long (nullable = true)\n |-- cdi: double (nullable = true)\n |-- mmi: double (nullable = true)\n |-- alert: string (nullable = true)\n |-- status: string (nullable = true)\n |-- tsunami: long (nullable = true)\n |-- sig: long (nullable = true)\n |-- net: string (nullable = true)\n |-- code: string (nullable = true)\n |-- ids: string (nullable = true)\n |-- sources: string (nullable = true)\n |-- types: string (nullable = true)\n |-- nst: long (nullable = true)\n |-- dmin: double (nullable = true)\n |-- rms: double (nullable = true)\n |-- gap: double (nullable = true)\n |-- magType: string (nullable = true)\n |-- type: string (nullable = true)\n |-- title: string (nullable = true)\n |-- longitude: double (nullable = true)\n |-- latitude: double (nullable = true)\n |-- depth: double (nullable = true)\n |-- ingestion_timestamp_utc: timestamp (nullable = true)\n\n"]}],"execution_count":3,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"b3537aff-7087-4c04-86a4-bcdbe9f577df"},{"cell_type":"code","source":["# --- Cell 4: Data Cleaning, Type Casting, Validation, and Deduplication ---\n","\n","# This cell performs crucial data quality and standardization steps to prepare the raw\n","# Bronze data for the Silver layer. This includes correcting data types, validating data\n","# integrity, and removing duplicate records.\n","\n","# 1. Type Casting and Column Renaming:\n","# This step converts raw data types (often string or generic numeric) into precise Spark types\n","# and renames columns to a consistent, more descriptive naming convention suitable for analytics.\n","df_cleaned = df_bronze.withColumn(\"event_timestamp_utc\", (F.col(\"time\") / 1000).cast(TimestampType())) \\\n","    .withColumn(\"updated_timestamp_utc\", (F.col(\"updated\") / 1000).cast(TimestampType())) \\\n","    .withColumn(\"magnitude\", F.col(\"mag\").cast(DoubleType())) \\\n","    .withColumn(\"depth_km\", F.col(\"depth\").cast(DoubleType())) \\\n","    .withColumn(\"latitude\", F.col(\"latitude\").cast(DoubleType())) \\\n","    .withColumn(\"longitude\", F.col(\"longitude\").cast(DoubleType())) \\\n","    .withColumn(\"tsunami_warning\", (F.col(\"tsunami\") == 1).cast(\"double\")) \\\n","    .withColumn(\"significance\", F.col(\"sig\").cast(IntegerType())) \\\n","    .withColumn(\"felt_reports\", F.col(\"felt\").cast(IntegerType())) \\\n","    .withColumn(\"nst_stations\", F.col(\"nst\").cast(IntegerType())) \\\n","    .withColumn(\"rms_travel_time\", F.col(\"rms\").cast(DoubleType())) \\\n","    .withColumn(\"gap_azimuthal\", F.col(\"gap\").cast(DoubleType()))\n","\n","logger.info(\"Initial type casting and column renaming complete, preparing for Silver layer schema.\")\n","\n","# 2. Select Final Columns for Silver Layer:\n","# After initial cleaning and casting, this step explicitly selects the columns that will be\n","# present in the Silver layer. This ensures a well-defined and consistent schema, removing\n","# unnecessary raw columns and finalizing naming conventions.\n","df_selected = df_cleaned.select(\n","    F.col(\"id\").alias(\"event_id\"),          # Renamed from 'id' to 'event_id' for clarity.\n","    \"event_timestamp_utc\",                  # Derived from 'time', cast to TimestampType.\n","    \"updated_timestamp_utc\",                # Derived from 'updated', cast to TimestampType.\n","    \"magnitude\",                            # Renamed from 'mag'.\n","    \"depth_km\",                             # Renamed from 'depth'.\n","    \"latitude\",\n","    \"longitude\",\n","    \"place\",\n","    F.col(\"type\").alias(\"event_type\"),      # Renamed from 'type' to 'event_type' to avoid keyword conflicts.\n","    \"magType\",\n","    \"tsunami_warning\",                      # Derived from 'tsunami', cast to BooleanType.\n","    \"significance\",                         # Renamed from 'sig'.\n","    \"felt_reports\",                         # Renamed from 'felt'.\n","    \"nst_stations\",                         # Renamed from 'nst'.\n","    \"rms_travel_time\",                      # Renamed from 'rms'.\n","    \"gap_azimuthal\",                        # Renamed from 'gap'.\n","    \"alert\",                                # Keeping original name 'alert' as it is descriptive.\n","    \"status\",\n","    \"url\",\n","    \"title\",\n","    \"ingestion_timestamp_utc\"               # Carry forward original ingestion timestamp for data lineage.\n",")\n","\n","logger.info(f\"Selected {len(df_selected.columns)} columns for the Silver layer schema.\")\n","\n","# 3. Data Validation and Filtering:\n","# Apply essential data quality checks to filter out invalid or erroneous records.\n","# This ensures that only high-quality, reliable data proceeds to the Silver layer,\n","# preventing downstream issues from malformed or out-of-range values.\n","initial_count_before_validation = df_selected.count()\n","df_validated = df_selected.filter(\n","    (F.col(\"magnitude\").isNotNull()) & (F.col(\"magnitude\").between(-2.0, 10.0)) & # Filter out invalid magnitudes.\n","    (F.col(\"latitude\").isNotNull()) & (F.col(\"latitude\").between(-90.0, 90.0)) &   # Validate latitude range.\n","    (F.col(\"longitude\").isNotNull()) & (F.col(\"longitude\").between(-180.0, 180.0)) & # Validate longitude range.\n","    (F.col(\"depth_km\").isNotNull()) & (F.col(\"depth_km\") >= 0) & (F.col(\"depth_km\") < 1000) & # Validate depth range (non-negative, realistic max).\n","    (F.col(\"event_timestamp_utc\").isNotNull()) & # Ensure event timestamp is present.\n","    (F.col(\"event_id\").isNotNull())              # Ensure event ID is present (primary key candidate).\n",")\n","records_removed_by_validation = initial_count_before_validation - df_validated.count()\n","if records_removed_by_validation > 0:\n","    logger.warning(f\"Removed {records_removed_by_validation} records due to data validation rules. Remaining records: {df_validated.count()}.\")\n","else:\n","    logger.info(\"All records passed data validation checks. No records were removed.\")\n","\n","# 4. Deduplication:\n","# Remove duplicate earthquake records based on a unique identifier (`event_id`),\n","# keeping the most recently updated version (`updated_timestamp_utc`).\n","from pyspark.sql.window import Window # Import Window class for window functions.\n","# Define a window specification: Partition by 'event_id' and order by 'updated_timestamp_utc' in descending order.\n","window_spec = Window.partitionBy(\"event_id\").orderBy(F.col(\"updated_timestamp_utc\").desc())\n","# Apply the window function to assign a row number within each partition.\n","# Then filter to keep only the first row (most recent update) for each 'event_id' and drop the row number column.\n","df_deduplicated = df_validated.withColumn(\"rn\", F.row_number().over(window_spec)).filter(F.col(\"rn\") == 1).drop(\"rn\")\n","\n","logger.info(f\"Records after cleaning, validation, and deduplication: {df_deduplicated.count()}.\")\n","if df_deduplicated.count() == 0:\n","    logger.warning(\"No records remaining after cleaning, validation, and deduplication. The Silver layer will be empty.\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":6,"statement_ids":[6],"state":"finished","livy_statement_state":"available","session_id":"f21af01b-40ba-416d-93ca-65d214b6f00e","normalized_state":"finished","queued_time":"2025-06-12T19:38:49.8477871Z","session_start_time":null,"execution_start_time":"2025-06-12T19:39:17.9938577Z","execution_finish_time":"2025-06-12T19:39:25.9819955Z","parent_msg_id":"d6109707-1560-49be-ab80-d16467fe6d17"},"text/plain":"StatementMeta(, f21af01b-40ba-416d-93ca-65d214b6f00e, 6, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stderr","text":["2025-06-12 19:39:18,267 - INFO - Initial type casting and column renaming complete, preparing for Silver layer schema.\n2025-06-12 19:39:18,300 - INFO - Selected 21 columns for the Silver layer schema.\n2025-06-12 19:39:21,646 - WARNING - Removed 82 records due to data validation rules. Remaining records: 24506.\n2025-06-12 19:39:23,583 - INFO - Records after cleaning, validation, and deduplication: 24506.\n"]}],"execution_count":4,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"7e234fc8-297a-47d0-9fcc-42668949a8e0"},{"cell_type":"code","source":["# --- Cell 5: Feature Engineering and Enrichment ---\n","\n","# This cell focuses on creating new derived features and enriching existing data within the\n","# Spark DataFrame. These additions typically make the data more useful for downstream analytical\n","# purposes and enhance its value, moving beyond just raw values.\n","\n","# 1. Categorical Feature Creation:\n","# These new columns categorize numerical values into more human-readable and analytically useful\n","# categorical bins.\n","# - `magnitude_category`: Classifies earthquakes into descriptive categories based on their magnitude\n","#                         (e.g., Micro, Minor, Light, Moderate, Strong, Major, Great).\n","# - `depth_category`: Categorizes earthquakes by depth (Shallow, Intermediate, Deep),\n","#                     which is relevant for geological analysis.\n","# - `hemisphere_ns`, `hemisphere_ew`: Determines the geographical hemisphere based on latitude/longitude,\n","#                                      useful for regional analysis.\n","df_enriched = df_deduplicated \\\n","    .withColumn(\"magnitude_category\",\n","        F.when(F.col(\"magnitude\") < 3.0, \"Micro\")\n","         .when(F.col(\"magnitude\") < 4.0, \"Minor\")\n","         .when(F.col(\"magnitude\") < 5.0, \"Light\")\n","         .when(F.col(\"magnitude\") < 6.0, \"Moderate\")\n","         .when(F.col(\"magnitude\") < 7.0, \"Strong\")\n","         .when(F.col(\"magnitude\") < 8.0, \"Major\")\n","         .otherwise(\"Great\")\n","    ) \\\n","    .withColumn(\"depth_category\",\n","        F.when(F.col(\"depth_km\") <= 70, \"Shallow\")\n","         .when(F.col(\"depth_km\") <= 300, \"Intermediate\")\n","         .otherwise(\"Deep\")\n","    ) \\\n","    .withColumn(\"hemisphere_ns\", F.when(F.col(\"latitude\") >= 0, \"Northern\").otherwise(\"Southern\")) \\\n","    .withColumn(\"hemisphere_ew\", F.when(F.col(\"longitude\") >= 0, \"Eastern\").otherwise(\"Western\"))\n","\n","# 2. Time-based Feature Extraction:\n","# Extract granular time components (year, month, day, hour, day of week) from the `event_timestamp_utc`.\n","# These are common dimensions used for time-series analysis, aggregation, and filtering.\n","df_enriched = df_enriched \\\n","    .withColumn(\"year\", F.year(F.col(\"event_timestamp_utc\"))) \\\n","    .withColumn(\"month\", F.month(F.col(\"event_timestamp_utc\"))) \\\n","    .withColumn(\"day\", F.dayofmonth(F.col(\"event_timestamp_utc\"))) \\\n","    .withColumn(\"hour\", F.hour(F.col(\"event_timestamp_utc\"))) \\\n","    .withColumn(\"day_of_week\", F.dayofweek(F.col(\"event_timestamp_utc\"))) # Note: In Spark, Sunday=1, Saturday=7.\n","\n","# 3. Geo-spatial Enrichment (Simplified Country/Region Extraction):\n","# This attempts to extract a country or major region from the 'place' string using a regular expression.\n","# It's a pragmatic approach for initial data, but with known limitations due to the variability\n","# of the 'place' field in the source API.\n","#\n","# This is a simplification. For robust geographical enrichment in production-grade\n","# data pipelines, consider the following more advanced and accurate approaches:\n","# - **Reverse Geocoding Services:** Integrate with external APIs (e.g., Google Maps API, HERE API, Geonames)\n","#   to get precise country, state, city, and administrative area information from latitude/longitude coordinates.\n","# - **Spatial Joins:** Perform spatial joins with a pre-existing geospatial dataset (e.g., shapefiles of country\n","#   boundaries, administrative regions) using dedicated geospatial libraries in Spark (e.g., GeoSpark, Sedona)\n","#   or by implementing custom User Defined Functions (UDFs).\n","# - **Advanced Pattern Matching/NLP:** Develop a more sophisticated rule-based system or leverage Natural Language\n","#   Processing (NLP) techniques to parse the `place` string, as its format can be highly inconsistent.\n","df_enriched = df_enriched.withColumn(\n","    \"extracted_region_detail\",\n","    F.trim(F.regexp_extract(F.col(\"place\"), r\",\\s*(.*)$\", 1)) # Extracts text after the last comma.\n",")\n","df_enriched = df_enriched.withColumn(\n","    \"extracted_country\",\n","    F.when(F.col(\"extracted_region_detail\") != \"\", F.col(\"extracted_region_detail\"))\n","     .otherwise(F.trim(F.col(\"place\"))) # If no comma is found, use the entire 'place' string as a fallback.\n",")\n","logger.warning(\"Simplified country/region extraction from 'place' string has been performed. For production environments, robust geocoding services or spatial joins are highly recommended for accuracy.\")\n","\n","# 4. Add Silver Processing Timestamp:\n","# Adds a timestamp indicating exactly when this record was processed into the Silver layer.\n","# This column (`silver_processing_timestamp_utc`) is vital for data lineage, auditing,\n","# and understanding the freshness of the data in the Silver layer.\n","df_enriched = df_enriched.withColumn(\"silver_processing_timestamp_utc\", PROCESSING_TIMESTAMP_UTC)\n","\n","logger.info(\"Feature engineering and enrichment complete.\")\n","logger.info(\"Sample of enriched data (first 5 records, showing new derived columns):\")\n","df_enriched.select(\"event_id\", \"magnitude_category\", \"depth_category\", \"extracted_country\", \"year\", \"month\").show(5, truncate=False)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":7,"statement_ids":[7],"state":"finished","livy_statement_state":"available","session_id":"f21af01b-40ba-416d-93ca-65d214b6f00e","normalized_state":"finished","queued_time":"2025-06-12T19:38:49.894445Z","session_start_time":null,"execution_start_time":"2025-06-12T19:39:25.9844792Z","execution_finish_time":"2025-06-12T19:39:28.3904975Z","parent_msg_id":"0a817674-6660-4b11-92ee-a2dd58207322"},"text/plain":"StatementMeta(, f21af01b-40ba-416d-93ca-65d214b6f00e, 7, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stderr","text":["2025-06-12 19:39:26,292 - WARNING - Simplified country/region extraction from 'place' string has been performed. For production environments, robust geocoding services or spatial joins are highly recommended for accuracy.\n2025-06-12 19:39:26,299 - INFO - Feature engineering and enrichment complete.\n2025-06-12 19:39:26,300 - INFO - Sample of enriched data (first 5 records, showing new derived columns):\n"]},{"output_type":"stream","name":"stdout","text":["+------------+------------------+--------------+-----------------+----+-----+\n|event_id    |magnitude_category|depth_category|extracted_country|year|month|\n+------------+------------------+--------------+-----------------+----+-----+\n|ak0247kxj8pd|Minor             |Shallow       |Alaska           |2024|6    |\n|ak0247kzewoq|Minor             |Shallow       |Alaska           |2024|6    |\n|ak0247kzvgsw|Micro             |Shallow       |Alaska           |2024|6    |\n|ak0247l1bkwu|Micro             |Shallow       |Alaska           |2024|6    |\n|ak0247l3pf99|Light             |Intermediate  |Alaska           |2024|6    |\n+------------+------------------+--------------+-----------------+----+-----+\nonly showing top 5 rows\n\n"]}],"execution_count":5,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"89da19fc-222a-4c12-a4cb-e86fef827a50"},{"cell_type":"code","source":["# --- Cell 6: Save to Silver Layer (Delta Table) ---\n","\n","# This cell is responsible for persisting the transformed and enriched data into the Silver layer\n","# of the Lakehouse. The Silver layer serves as a curated, high-quality data source optimized\n","# for analytical queries and consumption by downstream applications.\n","\n","# Determine the number of records to write. This check prevents attempting to write an empty DataFrame.\n","ecords_to_write_count = 0\n","if 'df_enriched' in locals(): # Check if the df_enriched DataFrame exists and is not empty.\n","    records_to_write_count = df_enriched.count()\n","\n","if records_to_write_count > 0: # Proceed only if there are records to write.\n","    try:\n","        # Write the Spark DataFrame to a Delta table in the Silver layer.\n","        # This leverages Delta Lake's capabilities for reliability, schema enforcement,\n","        # and performance within the Lakehouse environment (e.g., Azure Fabric).\n","        #\n","        # `format(\"delta\")`: Specifies the Delta Lake format, enabling ACID transactions,\n","        #                    schema evolution, and time travel.\n","        # `mode(\"overwrite\")`: Replaces the entire table with the new data. This is suitable\n","        #                      for full refreshes, common in initial loads or daily full snapshots\n","        #                      where the Bronze layer is fully re-ingested.\n","        #                      For incremental updates (processing only new or changed data),\n","        #                      `append` or `merge` (using `MERGE INTO` SQL or DataFrame API)\n","        #                      would be more appropriate.\n","        # `option(\"overwriteSchema\", \"true\")`: Allows the target Delta table's schema to be\n","        #                                       updated if the incoming DataFrame's schema differs.\n","        #                                       While useful for adapting to source schema changes,\n","        #                                       use with caution in production, especially for Silver/Gold\n","        #                                       layers where schema stability is often a priority.\n","        # `partitionBy(\"year\", \"month\")`: Partitions the data within the Delta table by the `year`\n","        #                                  and `month` of the earthquake event. This is a crucial\n","        #                                  optimization strategy for time-series data, as it significantly\n","        #                                  improves query performance when filtering by date ranges.\n","        # `saveAsTable(SILVER_TABLE_NAME)`: Registers the data as a named table within the Lakehouse\n","        #                                    metadata catalog, making it easily discoverable and queryable\n","        #                                    via Spark SQL or other integrated tools.\n","        logger.info(f\"Writing {df_enriched.count()} records to Silver table: {SILVER_TABLE_NAME}, partitioned by 'year' and 'month'.\")\n","        df_enriched.write \\\n","                   .format(\"delta\") \\\n","                   .mode(\"overwrite\") \\\n","                   .option(\"overwriteSchema\", \"true\") \\\n","                   .partitionBy(\"year\", \"month\") \\\n","                   .saveAsTable(SILVER_TABLE_NAME)\n","\n","        logger.info(f\"Successfully wrote {df_enriched.count()} records to Silver table: {SILVER_TABLE_NAME}.\")\n","\n","\n","    except Exception as e:\n","        logger.error(f\"FATAL ERROR: An error occurred while saving data to Silver table '{SILVER_TABLE_NAME}': {e}\", exc_info=True)\n","        raise Exception(f\"Transformation failed during Silver layer write: {e}\")\n","else:\n","    logger.warning(\"Skipping write to Silver layer as the enriched DataFrame is empty. No data was available to be written.\")\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":8,"statement_ids":[8],"state":"finished","livy_statement_state":"available","session_id":"f21af01b-40ba-416d-93ca-65d214b6f00e","normalized_state":"finished","queued_time":"2025-06-12T19:38:49.9541699Z","session_start_time":null,"execution_start_time":"2025-06-12T19:39:28.3929243Z","execution_finish_time":"2025-06-12T19:39:44.5254688Z","parent_msg_id":"41bc3313-19b3-496f-9ba5-03b3ef7dd00e"},"text/plain":"StatementMeta(, f21af01b-40ba-416d-93ca-65d214b6f00e, 8, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stderr","text":["2025-06-12 19:39:29,945 - INFO - Writing 24506 records to Silver table: silver_earthquakes_cleaned, partitioned by 'year' and 'month'.\n2025-06-12 19:39:43,486 - INFO - Successfully wrote 24506 records to Silver table: silver_earthquakes_cleaned.\n"]}],"execution_count":6,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"fa531551-ed58-4c83-b47b-6721f942c498"},{"cell_type":"code","source":["# --- Cell 7: Display Sample from Silver Table (Optional Verification) ---\n","\n","# This cell performs a quick verification step by querying the newly created\n","# or updated Silver Delta table and displaying a sample of its contents.\n","# This helps confirm that the data was written correctly, its schema is as expected\n","# after transformations, and that it is accessible within the Lakehouse environment.\n","# This step is optional but highly recommended during development and for validating deployments.\n","\n","# Check if `df_enriched` was successfully processed and records were intended to be written.\n","# This avoids attempting to query an empty or non-existent table if previous steps failed.\n","if 'records_to_write_count' in locals() and records_to_write_count > 0:\n","    try:\n","        # Ensure SparkSession is available to query the table.\n","        # This check is crucial if this cell were to be run in isolation or\n","        # after a Spark session might have expired or been reset.\n","        if 'spark' not in globals() or not isinstance(spark, SparkSession):\n","            logger.error(\"SparkSession 'spark' is not initialized for table verification. Attempting to get or create one.\")\n","            spark = SparkSession.builder.appName(\"EarthquakeSilverVerification\").getOrCreate()\n","            \n","        logger.info(f\"Displaying a sample of 5 records from the Silver table '{SILVER_TABLE_NAME}' for verification.\")\n","        # Retrieve the table as a Spark DataFrame using `spark.table()` and show its first 5 rows.\n","        # `truncate=False` ensures that column values are not truncated in the output,\n","        # providing a full view of the data for thorough verification.\n","        spark.table(SILVER_TABLE_NAME).show(5, truncate=False)\n","        \n","        # Log the schema and total count for further verification.\n","        # This provides a programmatic confirmation of the table's final structure and size.\n","        logger.info(f\"Silver table '{SILVER_TABLE_NAME}' schema:\")\n","        spark.table(SILVER_TABLE_NAME).printSchema()\n","        logger.info(f\"Total records in Silver table '{SILVER_TABLE_NAME}': {spark.table(SILVER_TABLE_NAME).count()}\")\n","\n","    except Exception as e:\n","        logger.error(f\"An error occurred while trying to read and display data from Silver table '{SILVER_TABLE_NAME}': {e}\", exc_info=True)\n","else:\n","    logger.info(\"Skipping Silver table display as no data was processed and written to the table in prior steps.\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":9,"statement_ids":[9],"state":"finished","livy_statement_state":"available","session_id":"f21af01b-40ba-416d-93ca-65d214b6f00e","normalized_state":"finished","queued_time":"2025-06-12T19:38:49.9935504Z","session_start_time":null,"execution_start_time":"2025-06-12T19:39:44.5282025Z","execution_finish_time":"2025-06-12T19:39:49.3573533Z","parent_msg_id":"30477709-aee0-4030-ae8a-7c7698ad595b"},"text/plain":"StatementMeta(, f21af01b-40ba-416d-93ca-65d214b6f00e, 9, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stderr","text":["2025-06-12 19:39:44,638 - INFO - Displaying a sample of 5 records from the Silver table 'silver_earthquakes_cleaned' for verification.\n2025-06-12 19:39:48,389 - INFO - Total records in Silver table 'silver_earthquakes_cleaned': 24506\n"]},{"output_type":"stream","name":"stdout","text":["+------------+-----------------------+-----------------------+---------+--------+--------+---------+-----------------------------------------+----------+-------+---------------+------------+------------+------------+---------------+-------------+-----+--------+--------------------------------------------------------------+-------------------------------------------------+--------------------------+------------------+--------------+-------------+-------------+----+-----+---+----+-----------+-----------------------+-----------------+-------------------------------+\n|event_id    |event_timestamp_utc    |updated_timestamp_utc  |magnitude|depth_km|latitude|longitude|place                                    |event_type|magType|tsunami_warning|significance|felt_reports|nst_stations|rms_travel_time|gap_azimuthal|alert|status  |url                                                           |title                                            |ingestion_timestamp_utc   |magnitude_category|depth_category|hemisphere_ns|hemisphere_ew|year|month|day|hour|day_of_week|extracted_region_detail|extracted_country|silver_processing_timestamp_utc|\n+------------+-----------------------+-----------------------+---------+--------+--------+---------+-----------------------------------------+----------+-------+---------------+------------+------------+------------+---------------+-------------+-----+--------+--------------------------------------------------------------+-------------------------------------------------+--------------------------+------------------+--------------+-------------+-------------+----+-----+---+----+-----------+-----------------------+-----------------+-------------------------------+\n|ak0248eo5yd5|2024-07-01 01:37:37.62 |2024-09-12 21:52:25.04 |2.7      |68.1    |59.8028 |-152.147 |17 km W of Anchor Point, Alaska          |earthquake|ml     |false          |112         |NULL        |NULL        |0.78           |NULL         |NULL |reviewed|https://earthquake.usgs.gov/earthquakes/eventpage/ak0248eo5yd5|M 2.7 - 17 km W of Anchor Point, Alaska          |2025-06-12 19:35:10.907037|Micro             |Shallow       |Northern     |Western      |2024|7    |1  |1   |2          |Alaska                 |Alaska           |2025-06-12 19:39:31.552488     |\n|ak0248gmx9wg|2024-07-02 20:22:14.901|2024-09-12 21:52:26.04 |2.5      |39.4    |57.7987 |-152.5636|4 km NNE of Kodiak Station, Alaska       |earthquake|ml     |false          |96          |NULL        |NULL        |0.46           |NULL         |NULL |reviewed|https://earthquake.usgs.gov/earthquakes/eventpage/ak0248gmx9wg|M 2.5 - 4 km NNE of Kodiak Station, Alaska       |2025-06-12 19:35:10.907037|Micro             |Shallow       |Northern     |Western      |2024|7    |2  |20  |3          |Alaska                 |Alaska           |2025-06-12 19:39:31.552488     |\n|ak0248i5p58m|2024-07-03 12:22:14.281|2024-08-10 07:13:12.669|3.2      |24.6    |65.2189 |-174.1082|98 km NNW of Provideniya, Russia         |earthquake|ml     |false          |158         |NULL        |NULL        |0.64           |NULL         |NULL |reviewed|https://earthquake.usgs.gov/earthquakes/eventpage/ak0248i5p58m|M 3.2 - 98 km NNW of Provideniya, Russia         |2025-06-12 19:35:10.907037|Minor             |Shallow       |Northern     |Western      |2024|7    |3  |12  |4          |Russia                 |Russia           |2025-06-12 19:39:31.552488     |\n|ak0248okrbyv|2024-07-07 00:31:44.41 |2024-09-12 21:52:30.04 |2.9      |8.9     |63.0854 |-151.4383|52 km SSE of Denali National Park, Alaska|earthquake|ml     |false          |129         |NULL        |NULL        |0.86           |NULL         |NULL |reviewed|https://earthquake.usgs.gov/earthquakes/eventpage/ak0248okrbyv|M 2.9 - 52 km SSE of Denali National Park, Alaska|2025-06-12 19:35:10.907037|Micro             |Shallow       |Northern     |Western      |2024|7    |7  |0   |1          |Alaska                 |Alaska           |2025-06-12 19:39:31.552488     |\n|ak0248rx0ply|2024-07-09 02:30:59.593|2024-09-20 16:00:52.04 |2.8      |66.6    |60.4922 |-151.4606|12 km NW of Kalifornsky, Alaska          |earthquake|ml     |false          |121         |NULL        |NULL        |0.68           |NULL         |NULL |reviewed|https://earthquake.usgs.gov/earthquakes/eventpage/ak0248rx0ply|M 2.8 - 12 km NW of Kalifornsky, Alaska          |2025-06-12 19:35:10.907037|Micro             |Shallow       |Northern     |Western      |2024|7    |9  |2   |3          |Alaska                 |Alaska           |2025-06-12 19:39:31.552488     |\n+------------+-----------------------+-----------------------+---------+--------+--------+---------+-----------------------------------------+----------+-------+---------------+------------+------------+------------+---------------+-------------+-----+--------+--------------------------------------------------------------+-------------------------------------------------+--------------------------+------------------+--------------+-------------+-------------+----+-----+---+----+-----------+-----------------------+-----------------+-------------------------------+\nonly showing top 5 rows\n\n"]}],"execution_count":7,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"95551602-0bc9-43f9-90a6-6adcfb664fb3"},{"cell_type":"code","source":[],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4187f55c-b38f-47af-9d17-1d40e8357a40"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"default_lakehouse":"d1339a60-7025-45d7-aaaf-2b1892c6a5d1","known_lakehouses":[{"id":"d1339a60-7025-45d7-aaaf-2b1892c6a5d1"}],"default_lakehouse_name":"EarthquakeDataLakehouse","default_lakehouse_workspace_id":"ed6f8caf-8e4a-4dc4-9934-0c0368d29949"}}},"nbformat":4,"nbformat_minor":5}