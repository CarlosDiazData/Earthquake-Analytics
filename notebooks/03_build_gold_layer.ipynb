{"cells":[{"cell_type":"code","source":["# --- Cell 1: Notebook Header, Logging Configuration, and Library Imports ---\n","\n","\"\"\"\n","Notebook: 03_build_gold_layer.ipynb\n","\n","Purpose:\n","This notebook processes cleaned and transformed earthquake data from the Silver layer\n","and loads it into a dimensional model (star schema) within the Lakehouse's Gold layer.\n","The Gold layer, stored as Delta tables within the Lakehouse, is highly refined,\n","aggregated, and conformed for optimized analytical consumption, reporting, and business intelligence.\n","It represents the final, business-ready data set.\n","\n","Dependencies:\n","- Python 3.x\n","- pyspark library (for distributed processing and DataFrame operations)\n","\n","Execution Environment:\n","This script is designed to run within an Apache Spark environment,\n","specifically optimized for platforms like Azure Fabric where a SparkSession\n","('spark') is typically pre-initialized and available globally.\n","\"\"\"\n","\n","# Configure a basic logging system for effective monitoring in production environments.\n","# This setup allows capturing informational messages, warnings, and errors throughout\n","# the script's execution, which is crucial for debugging and operational oversight.\n","import logging\n","logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n","logger = logging.getLogger(__name__)\n","\n","# PySpark libraries are imported for core DataFrame operations, built-in functions, and windowing.\n","from pyspark.sql import SparkSession            # The entry point for Spark functionality.\n","from pyspark.sql import functions as F          # Provides access to Spark SQL functions (e.g., F.col, F.year).\n","from pyspark.sql.window import Window           # Used for defining window specifications, e.g., for deduplication or ranking.\n","# Standard Python library for date and time manipulations.\n","from datetime import datetime, timedelta, date  # Used for generating the date dimension.\n","\n","# Initialize Spark Session:\n","# In Azure Fabric notebooks, the 'spark' session is typically pre-initialized and available globally.\n","# However, using `SparkSession.builder.getOrCreate()` is a robust pattern as it\n","# either retrieves the existing session or creates a new one if necessary,\n","# ensuring the notebook can run standalone or within a broader data pipeline.\n","try:\n","    if 'spark' not in globals() or not isinstance(spark, SparkSession):\n","        logger.info(\"SparkSession 'spark' not found or not an instance of SparkSession. Attempting to get or create a new one.\")\n","        spark = SparkSession.builder.appName(\"LoadLakehouseGold\").getOrCreate()\n","        logger.info(\"Spark session initialized successfully.\")\n","    else:\n","        logger.info(\"Spark session 'spark' is already initialized and available.\")\n","except Exception as e:\n","    logger.error(f\"FATAL ERROR: Failed to initialize or retrieve Spark session: {e}\", exc_info=True)\n","    raise Exception(f\"Failed to initialize Spark session: {e}\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":3,"statement_ids":[3],"state":"finished","livy_statement_state":"available","session_id":"df1f360c-6673-44aa-b572-53f9c07ecffb","normalized_state":"finished","queued_time":"2025-07-31T21:55:00.2366795Z","session_start_time":"2025-07-31T21:55:00.237581Z","execution_start_time":"2025-07-31T21:55:12.0000794Z","execution_finish_time":"2025-07-31T21:55:12.3855916Z","parent_msg_id":"fea9ddde-5d3b-4ff8-b321-1df7f275608c"},"text/plain":"StatementMeta(, df1f360c-6673-44aa-b572-53f9c07ecffb, 3, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stderr","text":["2025-07-31 21:55:12,116 - INFO - Spark session 'spark' is already initialized and available.\n"]}],"execution_count":1,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"fa6413f0-70ad-45b3-9b51-33ae7f171e2c"},{"cell_type":"code","source":["# --- Cell 2: Configuration Parameters ---\n","\n","# This section defines all key parameters for the Gold layer loading process,\n","# including the source (Silver) table name and the target (Gold) table names\n","# for the dimensional model. Centralizing these values ensures easy modification,\n","# maintainability, and consistency across the Gold layer components.\n","\n","# Source Table Name (Silver Layer):\n","# The fully qualified name of the Delta table in the Silver layer from which\n","# cleaned, validated, and semi-enriched data will be read to build the Gold layer.\n","SILVER_TABLE_NAME = \"silver_earthquakes_cleaned\"\n","\n","# Gold Layer Naming Conventions within the Lakehouse:\n","# For organizing Gold layer tables directly within the Lakehouse.\n","# This often involves a logical \"schema\" or prefix to group related tables\n","# belonging to a specific analytical domain or dimensional model.\n","GOLD_TABLE_PREFIX = \"gold_earthquake_\" # Prefix for all tables in this analytical model.\n","\n","# Construct full table names for each component of the Gold layer's dimensional model.\n","# This creates a clear mapping for where each dimension and fact table will reside.\n","GOLD_DIM_DATE_TABLE = f\"{GOLD_TABLE_PREFIX}dim_date\"\n","GOLD_DIM_LOCATION_TABLE = f\"{GOLD_TABLE_PREFIX}dim_location\"\n","GOLD_DIM_MAGNITUDE_TABLE = f\"{GOLD_TABLE_PREFIX}dim_magnitude\"\n","GOLD_DIM_EVENT_TYPE_TABLE = f\"{GOLD_TABLE_PREFIX}dim_event_type\"\n","GOLD_FACT_EVENTS_TABLE = f\"{GOLD_TABLE_PREFIX}fact_earthquake_events\"\n","\n","# Log the configured parameters for traceability and debugging in production environments.\n","logger.info(\"Gold Layer Load Configuration Loaded:\")\n","logger.info(f\"  Reading from Silver table: {SILVER_TABLE_NAME}\")\n","logger.info(f\"  Writing to Gold tables in Lakehouse, prefix: '{GOLD_TABLE_PREFIX}'):\")\n","logger.info(f\"    - DimDate: {GOLD_DIM_DATE_TABLE}\")\n","logger.info(f\"    - DimLocation: {GOLD_DIM_LOCATION_TABLE}\")\n","logger.info(f\"    - DimMagnitude: {GOLD_DIM_MAGNITUDE_TABLE}\")\n","logger.info(f\"    - DimEventType: {GOLD_DIM_EVENT_TYPE_TABLE}\")\n","logger.info(f\"    - FactEvents: {GOLD_FACT_EVENTS_TABLE}\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":4,"statement_ids":[4],"state":"finished","livy_statement_state":"available","session_id":"df1f360c-6673-44aa-b572-53f9c07ecffb","normalized_state":"finished","queued_time":"2025-07-31T21:55:00.2388036Z","session_start_time":null,"execution_start_time":"2025-07-31T21:55:12.3879923Z","execution_finish_time":"2025-07-31T21:55:12.7536836Z","parent_msg_id":"1dd2febc-ffef-4803-8e55-d1b81a6a3a3d"},"text/plain":"StatementMeta(, df1f360c-6673-44aa-b572-53f9c07ecffb, 4, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stderr","text":["2025-07-31 21:55:12,483 - INFO - Gold Layer Load Configuration Loaded:\n2025-07-31 21:55:12,484 - INFO -   Reading from Silver table: silver_earthquakes_cleaned\n2025-07-31 21:55:12,485 - INFO -   Writing to Gold tables in Lakehouse, prefix: 'gold_earthquake_'):\n2025-07-31 21:55:12,486 - INFO -     - DimDate: gold_earthquake_dim_date\n2025-07-31 21:55:12,486 - INFO -     - DimLocation: gold_earthquake_dim_location\n2025-07-31 21:55:12,487 - INFO -     - DimMagnitude: gold_earthquake_dim_magnitude\n2025-07-31 21:55:12,487 - INFO -     - DimEventType: gold_earthquake_dim_event_type\n2025-07-31 21:55:12,488 - INFO -     - FactEvents: gold_earthquake_fact_earthquake_events\n"]}],"execution_count":2,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"738d6d97-fd2e-48fe-9ade-260f750adbfb"},{"cell_type":"code","source":["# --- Cell 3: Load Silver Data ---\n","\n","# This cell loads the pre-processed earthquake data from the Silver layer\n","# into a PySpark DataFrame. This DataFrame serves as the primary source\n","# for extracting data to populate the various dimension and fact tables in the Gold layer.\n","\n","try:\n","    # Read the Silver layer Delta table into a Spark DataFrame.\n","    # This table contains validated, cleaned, and enriched data from the previous transformation step,\n","    # making it suitable for direct consumption by the Gold layer.\n","    df_silver = spark.table(SILVER_TABLE_NAME)\n","    \n","    # Materialize the count of records to log and check for emptiness.\n","    silver_record_count = df_silver.count()\n","    \n","    logger.info(f\"Successfully loaded {silver_record_count} records from Silver table: {SILVER_TABLE_NAME}.\")\n","    logger.info(\"Silver DataFrame Schema:\")\n","    df_silver.printSchema()\n","\n","    # Check if the Silver table is empty.\n","    # If the source data for the Gold layer is empty, subsequent dimensional modeling\n","    # and fact table population steps will also result in empty tables.\n","    if silver_record_count == 0:\n","        logger.warning(f\"Silver table '{SILVER_TABLE_NAME}' is empty. No data to load into the Gold layer.\")\n","        # Create an empty DataFrame with the expected schema to prevent subsequent errors\n","        # in transformations that expect a DataFrame, ensuring the pipeline can proceed gracefully.\n","        df_silver = spark.createDataFrame([], df_silver.schema)\n","        \n","except Exception as e:\n","    logger.error(f\"FATAL ERROR: Failed to load Silver data from '{SILVER_TABLE_NAME}'. Gold layer loading cannot proceed. Error: {e}\", exc_info=True)\n","    raise Exception(f\"Failed to load Silver data: {e}\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":5,"statement_ids":[5],"state":"finished","livy_statement_state":"available","session_id":"df1f360c-6673-44aa-b572-53f9c07ecffb","normalized_state":"finished","queued_time":"2025-07-31T21:55:00.2406984Z","session_start_time":null,"execution_start_time":"2025-07-31T21:55:12.7559507Z","execution_finish_time":"2025-07-31T21:55:26.33196Z","parent_msg_id":"aff1a08f-8296-42b7-ad28-2aa784132827"},"text/plain":"StatementMeta(, df1f360c-6673-44aa-b572-53f9c07ecffb, 5, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["root\n |-- event_id: string (nullable = true)\n |-- event_timestamp_utc: timestamp (nullable = true)\n |-- updated_timestamp_utc: timestamp (nullable = true)\n |-- magnitude: double (nullable = true)\n |-- depth_km: double (nullable = true)\n |-- latitude: double (nullable = true)\n |-- longitude: double (nullable = true)\n |-- place: string (nullable = true)\n |-- event_type: string (nullable = true)\n |-- magType: string (nullable = true)\n |-- tsunami_warning: double (nullable = true)\n |-- significance: integer (nullable = true)\n |-- felt_reports: integer (nullable = true)\n |-- nst_stations: integer (nullable = true)\n |-- rms_travel_time: double (nullable = true)\n |-- gap_azimuthal: double (nullable = true)\n |-- alert: string (nullable = true)\n |-- status: string (nullable = true)\n |-- url: string (nullable = true)\n |-- title: string (nullable = true)\n |-- ingestion_timestamp_utc: timestamp (nullable = true)\n |-- magnitude_category: string (nullable = true)\n |-- depth_category: string (nullable = true)\n |-- hemisphere_ns: string (nullable = true)\n |-- hemisphere_ew: string (nullable = true)\n |-- year: integer (nullable = true)\n |-- month: integer (nullable = true)\n |-- day: integer (nullable = true)\n |-- hour: integer (nullable = true)\n |-- day_of_week: integer (nullable = true)\n |-- extracted_region_detail: string (nullable = true)\n |-- extracted_country: string (nullable = true)\n |-- silver_processing_timestamp_utc: timestamp (nullable = true)\n\n"]}],"execution_count":3,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"2f27696c-a125-4ae7-a577-ea1c7fc54820"},{"cell_type":"code","source":["# --- Cell 4: Populate DimDate Dimension Table ---\n","\n","# This cell generates and populates the `DimDate` dimension table.\n","\n","# Check if `df_silver` is empty or not defined before proceeding with date range determination.\n","if 'df_silver' not in locals() or df_silver.count() == 0:\n","    logger.warning(\"df_silver is empty or not defined. Skipping DimDate population.\")\n","    # Create an empty DataFrame with a predefined schema to ensure consistency for downstream operations,\n","    # even if no data is generated for DimDate. The schema is implied by the `date_list` structure.\n","    df_dim_date = spark.createDataFrame([], schema=\"DateKey INT, FullDate DATE, Year INT, Quarter INT, Month INT, MonthName STRING, DayOfMonth INT, DayOfWeek INT, DayName STRING, IsWeekend INT, ISOWeekOfYear INT\") \n","else:\n","    logger.info(\"Determining date range for DimDate population from Silver data's event timestamps.\")\n","    # Determine the minimum and maximum event dates from the `event_timestamp_utc` column in the Silver data.\n","    min_max_date_row = df_silver.select(F.min(\"event_timestamp_utc\").alias(\"min_date\"), \n","                                        F.max(\"event_timestamp_utc\").alias(\"max_date\")).first()\n","\n","    # Define the precise start and end dates for the `DimDate` generation.\n","    # Add a buffer (e.g., 30 days) to the `max_date` to account for potential future events or late-arriving data.\n","    # Include a fallback to a predefined historical and future range if no valid dates are found in `df_silver`.\n","    if min_max_date_row and min_max_date_row[\"min_date\"] and min_max_date_row[\"max_date\"]:\n","        start_date: date = min_max_date_row[\"min_date\"].date()\n","        end_date: date = min_max_date_row[\"max_date\"].date() + timedelta(days=30) # Add a 30-day buffer.\n","    else: \n","        logger.warning(\"No valid min/max dates found in df_silver. Using default date range (2020-01-01 to 1 year from now) for DimDate.\")\n","        start_date: date = datetime(2020, 1, 1).date()        # Default start from a reasonable historical date.\n","        end_date: date = datetime.now().date() + timedelta(days=365) # Default projection for a year into the future.\n","\n","    logger.info(f\"Generating DimDate from {start_date} to {end_date}.\")\n","\n","    # Generate a list of dictionaries, where each dictionary represents a day and its attributes.\n","    # This programmatic generation ensures all required date dimensions are present.\n","    date_list = []\n","    current_date: date = start_date\n","    while current_date <= end_date:\n","        date_list.append({\n","            'DateKey': int(current_date.strftime('%Y%m%d')), # YYYYMMDD as an INTEGER for efficient joins in data warehouses.\n","            'FullDate': current_date,                       # Full date as a Python date object.\n","            'Year': current_date.year,\n","            'Quarter': (current_date.month - 1) // 3 + 1,   # Calculate quarter (1-4).\n","            'Month': current_date.month,\n","            'MonthName': current_date.strftime('%B'),       # Full month name (e.g., \"January\").\n","            'DayOfMonth': current_date.day,\n","            # DayOfWeek: 1 (Sunday) to 7 (Saturday) to match common SQL DATEPART(dw,...) conventions.\n","            'DayOfWeek': current_date.isoweekday() % 7 + 1, # Monday=0, Sunday=6 in Python's weekday().\n","            'DayName': current_date.strftime('%A'),         # Full day name (e.g., \"Monday\").\n","            'IsWeekend': 1 if current_date.weekday() >= 5 else 0, # 1 for Saturday/Sunday, 0 otherwise.\n","            'ISOWeekOfYear': current_date.isocalendar()[1]  # ISO week number.\n","        })\n","        current_date += timedelta(days=1)                   # Increment to the next day.\n","\n","    # Create Spark DataFrame from the list of date attributes.\n","    df_dim_date = spark.createDataFrame(date_list)\n","    \n","    # Persist DimDate to the Gold layer of the Lakehouse.\n","    # 'overwrite' mode is used here because DimDate is typically a static or slowly changing table\n","    # that can be fully rebuilt without losing historical context, or it follows a specific lifecycle.\n","    # 'overwriteSchema' allows schema evolution (e.g., adding new date attributes) if the definition changes.\n","    try:\n","        df_dim_date.write \\\n","                   .format(\"delta\") \\\n","                   .mode(\"overwrite\") \\\n","                   .option(\"overwriteSchema\", \"true\") \\\n","                   .saveAsTable(GOLD_DIM_DATE_TABLE)\n","        logger.info(f\"DimDate populated successfully with {df_dim_date.count()} records into table: {GOLD_DIM_DATE_TABLE}.\")\n","        logger.info(\"Sample records from DimDate:\")\n","        df_dim_date.show(5, truncate=False)\n","    except Exception as e:\n","        logger.error(f\"Error populating DimDate to Gold layer table '{GOLD_DIM_DATE_TABLE}': {e}\", exc_info=True)\n","        raise Exception(f\"Failed to populate DimDate: {e}\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":6,"statement_ids":[6],"state":"finished","livy_statement_state":"available","session_id":"df1f360c-6673-44aa-b572-53f9c07ecffb","normalized_state":"finished","queued_time":"2025-07-31T21:55:00.2426877Z","session_start_time":null,"execution_start_time":"2025-07-31T21:55:26.334345Z","execution_finish_time":"2025-07-31T21:55:40.5980362Z","parent_msg_id":"3872ad5e-616e-4e2c-9ded-a3dd4d4f55ae"},"text/plain":"StatementMeta(, df1f360c-6673-44aa-b572-53f9c07ecffb, 6, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stderr","text":["2025-07-31 21:55:27,104 - INFO - Determining date range for DimDate population from Silver data's event timestamps.\n2025-07-31 21:55:29,734 - INFO - Generating DimDate from 2024-07-30 to 2025-08-29.\n2025-07-31 21:55:38,151 - INFO - DimDate populated successfully with 396 records into table: gold_earthquake_dim_date.\n2025-07-31 21:55:38,153 - INFO - Sample records from DimDate:\n"]},{"output_type":"stream","name":"stdout","text":["+--------+---------+----------+---------+----------+-------------+---------+-----+---------+-------+----+\n|DateKey |DayName  |DayOfMonth|DayOfWeek|FullDate  |ISOWeekOfYear|IsWeekend|Month|MonthName|Quarter|Year|\n+--------+---------+----------+---------+----------+-------------+---------+-----+---------+-------+----+\n|20240730|Tuesday  |30        |3        |2024-07-30|31           |0        |7    |July     |3      |2024|\n|20240731|Wednesday|31        |4        |2024-07-31|31           |0        |7    |July     |3      |2024|\n|20240801|Thursday |1         |5        |2024-08-01|31           |0        |8    |August   |3      |2024|\n|20240802|Friday   |2         |6        |2024-08-02|31           |0        |8    |August   |3      |2024|\n|20240803|Saturday |3         |7        |2024-08-03|31           |1        |8    |August   |3      |2024|\n+--------+---------+----------+---------+----------+-------------+---------+-----+---------+-------+----+\nonly showing top 5 rows\n\n"]}],"execution_count":4,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"233f2e69-f5bf-4bd8-9ce9-93d1f6bf7fa8"},{"cell_type":"code","source":["# --- Cell 5: Populate DimLocation Dimension Table ---\n","\n","# This cell populates the `DimLocation` dimension table.\n","# It extracts distinct location attributes (latitude, longitude, place description, extracted country/region, hemisphere)\n","# from the Silver layer and assigns a unique surrogate key for each unique combination.\n","\n","# Check if `df_silver` is empty or not defined before proceeding with transformations.\n","if 'df_silver' not in locals() or df_silver.count() == 0:\n","    logger.warning(\"df_silver is empty or not defined. Skipping DimLocation population.\")\n","    # Create an empty DataFrame with a placeholder schema for consistency.\n","    df_dim_location = spark.createDataFrame([], schema=\"LocationKey LONG, Latitude DOUBLE, Longitude DOUBLE, PlaceDescription STRING, ExtractedCountry STRING, ExtractedRegionDetail STRING, HemisphereNS STRING, HemisphereEW STRING\")\n","else:\n","    logger.info(\"Extracting distinct location attributes for DimLocation population.\")\n","    # Select distinct location attributes from the Silver DataFrame.\n","    # This ensures that each unique combination of geographical details gets a single entry in the dimension table.\n","    df_dim_location_source = df_silver.select(\n","        \"latitude\", \"longitude\", \"place\", \n","        \"extracted_country\", \"extracted_region_detail\",\n","        \"hemisphere_ns\", \"hemisphere_ew\"\n","    ).distinct()\n","\n","    # Assign a surrogate key (`LocationKey`) to each unique location record.\n","    df_dim_location = df_dim_location_source.withColumn(\"LocationKey\", F.monotonically_increasing_id() + 1) \\\n","        .select(\n","            F.col(\"LocationKey\"),\n","            F.col(\"latitude\").alias(\"Latitude\"),\n","            F.col(\"longitude\").alias(\"Longitude\"),\n","            F.col(\"place\").alias(\"PlaceDescription\"),\n","            F.col(\"extracted_country\").alias(\"ExtractedCountry\"),\n","            F.col(\"extracted_region_detail\").alias(\"ExtractedRegionDetail\"),\n","            F.col(\"hemisphere_ns\").alias(\"HemisphereNS\"),\n","            F.col(\"hemisphere_ew\").alias(\"HemisphereEW\")\n","        )\n","\n","    # Persist DimLocation to the Gold layer of the Lakehouse.\n","    # 'overwrite' mode is used for simplicity, assuming a full rebuild is acceptable for this dimension.\n","    try:\n","        df_dim_location.write \\\n","                       .format(\"delta\") \\\n","                       .mode(\"overwrite\") \\\n","                       .option(\"overwriteSchema\", \"true\") \\\n","                       .saveAsTable(GOLD_DIM_LOCATION_TABLE)\n","        logger.info(f\"DimLocation populated successfully with {df_dim_location.count()} records into table: {GOLD_DIM_LOCATION_TABLE}.\")\n","        logger.info(\"Sample records from DimLocation:\")\n","        df_dim_location.show(5, truncate=False)\n","    except Exception as e:\n","        logger.error(f\"Error populating DimLocation to Gold layer table '{GOLD_DIM_LOCATION_TABLE}': {e}\", exc_info=True)\n","        raise Exception(f\"Failed to populate DimLocation: {e}\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":7,"statement_ids":[7],"state":"finished","livy_statement_state":"available","session_id":"df1f360c-6673-44aa-b572-53f9c07ecffb","normalized_state":"finished","queued_time":"2025-07-31T21:55:00.2444556Z","session_start_time":null,"execution_start_time":"2025-07-31T21:55:40.6004509Z","execution_finish_time":"2025-07-31T21:55:50.536175Z","parent_msg_id":"03abba3e-584f-400d-907f-cf51758f669f"},"text/plain":"StatementMeta(, df1f360c-6673-44aa-b572-53f9c07ecffb, 7, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stderr","text":["2025-07-31 21:55:41,179 - INFO - Extracting distinct location attributes for DimLocation population.\n"]},{"output_type":"stream","name":"stdout","text":["+-----------+----------------+-----------------+---------------------------------------+----------------+---------------------+------------+------------+\n|LocationKey|Latitude        |Longitude        |PlaceDescription                       |ExtractedCountry|ExtractedRegionDetail|HemisphereNS|HemisphereEW|\n+-----------+----------------+-----------------+---------------------------------------+----------------+---------------------+------------+------------+\n|1          |31.5118333      |-114.3651667     |97 km SE of Estacion Coahuila, B.C., MX|B.C., MX        |B.C., MX             |Northern    |Western     |\n|2          |19.3888333333333|-155.277333333333|7 km SW of Volcano, Hawaii             |Hawaii          |Hawaii               |Northern    |Western     |\n|3          |-21.9587        |-63.7892         |13 km WNW of Yacuiba, Bolivia          |Bolivia         |Bolivia              |Southern    |Western     |\n|4          |-17.4186        |-178.8261        |209 km ENE of Levuka, Fiji             |Fiji            |Fiji                 |Southern    |Western     |\n|5          |40.3753         |24.1411          |14 km ENE of Ouranoupolis, Greece      |Greece          |Greece               |Northern    |Eastern     |\n+-----------+----------------+-----------------+---------------------------------------+----------------+---------------------+------------+------------+\nonly showing top 5 rows\n\n"]}],"execution_count":5,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"bca85e0f-b3d8-4fdf-8205-26355daaf3aa"},{"cell_type":"code","source":["# --- Cell 6: Populate DimMagnitude Dimension Table ---\n","\n","# This cell populates the `DimMagnitude` dimension table.\n","# This is typically a static dimension, meaning its data (magnitude categories, descriptions)\n","# is predefined and does not change based on the source data.\n","# It provides descriptive context for earthquake magnitudes, making analytical queries more readable.\n","\n","logger.info(\"Populating DimMagnitude with static predefined categories.\")\n","\n","# Define the static data for magnitude categories.\n","# This data is hardcoded as it represents a fixed business classification of earthquake magnitudes.\n","magnitude_categories_data = [\n","    {\"MagnitudeCategory\": \"Micro\", \"MinMagnitude\": -2.0, \"MaxMagnitude\": 2.9, \"Description\": \"Microearthquakes, not felt, or felt rarely by sensitive people.\"},\n","    {\"MagnitudeCategory\": \"Minor\", \"MinMagnitude\": 3.0, \"MaxMagnitude\": 3.9, \"Description\": \"Often felt by people, but very rarely causes damage.\"},\n","    {\"MagnitudeCategory\": \"Light\", \"MinMagnitude\": 4.0, \"MaxMagnitude\": 4.9, \"Description\": \"Felt indoors by many, outdoors by few. Light damage possible.\"},\n","    {\"MagnitudeCategory\": \"Moderate\", \"MinMagnitude\": 5.0, \"MaxMagnitude\": 5.9, \"Description\": \"Felt by everyone. Slight damage to weak structures.\"},\n","    {\"MagnitudeCategory\": \"Strong\", \"MinMagnitude\": 6.0, \"MaxMagnitude\": 6.9, \"Description\": \"Damage to a moderate number of well-built structures.\"},\n","    {\"MagnitudeCategory\": \"Major\", \"MinMagnitude\": 7.0, \"MaxMagnitude\": 7.9, \"Description\": \"Causes damage to most buildings, some toppling.\"},\n","    {\"MagnitudeCategory\": \"Great\", \"MinMagnitude\": 8.0, \"MaxMagnitude\": 10.0, \"Description\": \"Causes widespread destruction, severe damage or collapse.\"},\n","    {\"MagnitudeCategory\": \"Unknown\", \"MinMagnitude\": None, \"MaxMagnitude\": None, \"Description\": \"Magnitude category could not be determined.\"} # Fallback for unexpected values.\n","]\n","df_dim_magnitude_static = spark.createDataFrame(magnitude_categories_data)\n","\n","# Assign a surrogate key (`MagnitudeKey`) to each magnitude category.\n","# For static dimensions that are fully overwritten, `monotonically_increasing_id()` is acceptable,\n","# as key consistency across runs is not a strict requirement for a dimension that is always rebuilt.\n","df_dim_magnitude = df_dim_magnitude_static.withColumn(\"MagnitudeKey\", F.monotonically_increasing_id() + 1) \\\n","    .select(\"MagnitudeKey\", \"MagnitudeCategory\", \"MinMagnitude\", \"MaxMagnitude\", \"Description\")\n","\n","# Persist DimMagnitude to the Gold layer of the Lakehouse.\n","# 'overwrite' mode is appropriate for static or very slowly changing dimensions where a full refresh\n","# of the lookup data is always desired.\n","try:\n","    df_dim_magnitude.write \\\n","                    .format(\"delta\") \\\n","                    .mode(\"overwrite\") \\\n","                    .option(\"overwriteSchema\", \"true\") \\\n","                    .saveAsTable(GOLD_DIM_MAGNITUDE_TABLE)\n","    logger.info(f\"DimMagnitude populated successfully with {df_dim_magnitude.count()} records into table: {GOLD_DIM_MAGNITUDE_TABLE}.\")\n","    logger.info(\"Sample records from DimMagnitude:\")\n","    df_dim_magnitude.show(truncate=False) # Show all rows as it's a small static table.\n","except Exception as e:\n","    logger.error(f\"Error populating DimMagnitude to Gold layer table '{GOLD_DIM_MAGNITUDE_TABLE}': {e}\", exc_info=True)\n","    raise Exception(f\"Failed to populate DimMagnitude: {e}\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":8,"statement_ids":[8],"state":"finished","livy_statement_state":"available","session_id":"df1f360c-6673-44aa-b572-53f9c07ecffb","normalized_state":"finished","queued_time":"2025-07-31T21:55:00.2463313Z","session_start_time":null,"execution_start_time":"2025-07-31T21:55:50.5384171Z","execution_finish_time":"2025-07-31T21:55:56.8563769Z","parent_msg_id":"20e97a15-518c-43f1-a4df-7abba4a10f64"},"text/plain":"StatementMeta(, df1f360c-6673-44aa-b572-53f9c07ecffb, 8, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stderr","text":["2025-07-31 21:55:50,644 - INFO - Populating DimMagnitude with static predefined categories.\n"]},{"output_type":"stream","name":"stdout","text":["+------------+-----------------+------------+------------+---------------------------------------------------------------+\n|MagnitudeKey|MagnitudeCategory|MinMagnitude|MaxMagnitude|Description                                                    |\n+------------+-----------------+------------+------------+---------------------------------------------------------------+\n|1           |Micro            |-2.0        |2.9         |Microearthquakes, not felt, or felt rarely by sensitive people.|\n|8589934593  |Minor            |3.0         |3.9         |Often felt by people, but very rarely causes damage.           |\n|17179869185 |Light            |4.0         |4.9         |Felt indoors by many, outdoors by few. Light damage possible.  |\n|25769803777 |Moderate         |5.0         |5.9         |Felt by everyone. Slight damage to weak structures.            |\n|34359738369 |Strong           |6.0         |6.9         |Damage to a moderate number of well-built structures.          |\n|42949672961 |Major            |7.0         |7.9         |Causes damage to most buildings, some toppling.                |\n|51539607553 |Great            |8.0         |10.0        |Causes widespread destruction, severe damage or collapse.      |\n|60129542145 |Unknown          |NULL        |NULL        |Magnitude category could not be determined.                    |\n+------------+-----------------+------------+------------+---------------------------------------------------------------+\n\n"]}],"execution_count":6,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"031e33a4-8754-4aae-9f13-f4ec2a16dccc"},{"cell_type":"code","source":["# --- Cell 7: Populate DimEventType Dimension Table ---\n","\n","# This cell populates the `DimEventType` dimension table.\n","# It extracts distinct combinations of `event_type` and `MagType` from the Silver layer\n","# to create a comprehensive lookup for different event classifications and their associated\n","# magnitude types.\n","\n","# Check if `df_silver` is empty or not defined before proceeding.\n","if 'df_silver' not in locals() or df_silver.count() == 0:\n","    logger.warning(\"df_silver is empty or not defined. Skipping DimEventType population.\")\n","    # Create an empty DataFrame with a placeholder schema for consistency.\n","    df_dim_event_type = spark.createDataFrame([], schema=\"EventTypeKey LONG, EventType STRING, MagType STRING\")\n","else:\n","    logger.info(\"Extracting distinct event types and magnitude types for DimEventType population.\")\n","    # Select distinct combinations of `event_type` and `MagType` from the Silver DataFrame.\n","    # This ensures that each unique combination is represented only once in the dimension.\n","    df_dim_event_type_source = df_silver.select(\"event_type\", \"MagType\").distinct()\n","\n","    # Assign a surrogate key (`EventTypeKey`) to each unique event type combination.\n","    # Similar considerations for `monotonically_increasing_id()` apply here as in `DimLocation`\n","    # regarding consistency across runs vs. full overwrite.\n","    df_dim_event_type = df_dim_event_type_source.withColumn(\"EventTypeKey\", F.monotonically_increasing_id() + 1) \\\n","        .select(\n","            F.col(\"EventTypeKey\"),\n","            F.col(\"event_type\").alias(\"EventType\"), # Alias for clarity in the dimension.\n","            F.col(\"MagType\")\n","        )\n","\n","    # Persist DimEventType to the Gold layer of the Lakehouse.\n","    # 'overwrite' mode is used, suitable for dimensions that are fully refreshed or when\n","    # the entire set of event types is expected to be rebuilt.\n","    try:\n","        df_dim_event_type.write \\\n","                         .format(\"delta\") \\\n","                         .mode(\"overwrite\") \\\n","                         .option(\"overwriteSchema\", \"true\") \\\n","                         .saveAsTable(GOLD_DIM_EVENT_TYPE_TABLE)\n","        logger.info(f\"DimEventType populated successfully with {df_dim_event_type.count()} records into table: {GOLD_DIM_EVENT_TYPE_TABLE}.\")\n","        logger.info(\"Sample records from DimEventType:\")\n","        df_dim_event_type.show(truncate=False)\n","    except Exception as e:\n","        logger.error(f\"Error populating DimEventType to Gold layer table '{GOLD_DIM_EVENT_TYPE_TABLE}': {e}\", exc_info=True)\n","        raise Exception(f\"Failed to populate DimEventType: {e}\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":9,"statement_ids":[9],"state":"finished","livy_statement_state":"available","session_id":"df1f360c-6673-44aa-b572-53f9c07ecffb","normalized_state":"finished","queued_time":"2025-07-31T21:55:00.248148Z","session_start_time":null,"execution_start_time":"2025-07-31T21:55:56.8585224Z","execution_finish_time":"2025-07-31T21:56:05.1527242Z","parent_msg_id":"2656dc1c-f852-44dc-a25e-b4f3f37401e7"},"text/plain":"StatementMeta(, df1f360c-6673-44aa-b572-53f9c07ecffb, 9, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stderr","text":["2025-07-31 21:55:57,517 - INFO - Extracting distinct event types and magnitude types for DimEventType population.\n2025-07-31 21:56:02,807 - INFO - DimEventType populated successfully with 15 records into table: gold_earthquake_dim_event_type.\n2025-07-31 21:56:02,808 - INFO - Sample records from DimEventType:\n"]},{"output_type":"stream","name":"stdout","text":["+------------+-----------------+-------+\n|EventTypeKey|EventType        |MagType|\n+------------+-----------------+-------+\n|1           |earthquake       |mww    |\n|2           |earthquake       |mwr    |\n|3           |earthquake       |mw     |\n|4           |earthquake       |ml     |\n|5           |earthquake       |md     |\n|6           |earthquake       |mb     |\n|7           |mining explosion |ml     |\n|8           |earthquake       |mb_lg  |\n|9           |earthquake       |mh     |\n|10          |landslide        |ms_vx  |\n|11          |volcanic eruption|ml     |\n|12          |explosion        |mb     |\n|13          |explosion        |ml     |\n|14          |other event      |ml     |\n|15          |earthquake       |mwb    |\n+------------+-----------------+-------+\n\n"]}],"execution_count":7,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"20c55dd2-7de5-4b7a-91ef-ee7fef6a3fcb"},{"cell_type":"code","source":["# --- Cell 8: Prepare Fact Table Data (Joins and Surrogate Key Lookups) ---\n","\n","# This cell prepares the data for the `FactEarthquakeEvents` table.\n","# It involves joining the processed Silver layer data with the newly populated dimension tables\n","# (DimDate, DimLocation, DimMagnitude, DimEventType). \n","\n","# Check if `df_silver` is empty or not defined before proceeding.\n","if 'df_silver' not in locals() or df_silver.count() == 0:\n","    logger.warning(\"df_silver is empty or not defined. Skipping Fact table preparation.\")\n","    # Create an empty DataFrame with a placeholder schema for consistency.\n","    # The schema should match the expected output of `df_fact_final`.\n","    df_fact_final = spark.createDataFrame([], schema=\"EarthquakeEventKey LONG, EventID STRING, DateKey INT, TimeOfDay STRING, LocationKey LONG, MagnitudeKey LONG, EventTypeKey LONG, Magnitude DOUBLE, DepthKm DOUBLE, TsunamiWarning BOOLEAN, Significance INT, FeltReports INT, NumberOfStations INT, RmsTravelTime DOUBLE, AzimuthalGap DOUBLE, SourceURL STRING, SilverProcessingTimestampUTC TIMESTAMP, DWLoadTimestampUTC TIMESTAMP\")\n","else:\n","    logger.info(\"Loading dimension tables from Gold layer for fact table joins.\")\n","    # Load Dimension tables back into Spark DataFrames from the Gold layer.\n","    # This ensures we are joining against the most current and accurate state of the dimensions.\n","    try:\n","        df_dim_date_loaded = spark.table(GOLD_DIM_DATE_TABLE)\n","        df_dim_location_loaded = spark.table(GOLD_DIM_LOCATION_TABLE)\n","        df_dim_magnitude_loaded = spark.table(GOLD_DIM_MAGNITUDE_TABLE)\n","        df_dim_event_type_loaded = spark.table(GOLD_DIM_EVENT_TYPE_TABLE)\n","        logger.info(\"Dimension tables loaded successfully from Gold layer for joining.\")\n","    except Exception as e:\n","        # If any dimension table cannot be loaded, the fact table cannot be properly built.\n","        logger.error(f\"FATAL ERROR: Failed to load dimension tables from Gold layer. Fact table preparation cannot proceed. Error: {e}\", exc_info=True)\n","        raise Exception(f\"Failed to load dimensions for fact table: {e}\")\n","\n","    # Prepare specific columns from the Silver DataFrame for joining and inclusion in the fact table.\n","    # - `FactDateKey`: Derived from `event_timestamp_utc` and cast to INT for joining with `DimDate`.\n","    # - `TimeOfDayString`: Formatted to \"HH:mm:ss.SSSSSS\" as a string. Spark does not have a native SQL `TIME` type,\n","    #   so storing it as a string is a common workaround when targeting a data warehouse that supports `TIME(6)`.\n","    df_fact_source = df_silver \\\n","        .withColumn(\"FactDateKey\", F.date_format(F.col(\"event_timestamp_utc\"), \"yyyyMMdd\").cast(\"int\")) \\\n","        .withColumn(\"TimeOfDayString\", F.date_format(F.col(\"event_timestamp_utc\"), \"HH:mm:ss.SSSSSS\"))\n","\n","    logger.info(\"Performing joins with dimension tables to resolve surrogate keys for the fact table.\")\n","    # Join with DimDate: Link by the generated `DateKey`. An `inner` join ensures that only\n","    # event records that have a corresponding date entry in `DimDate` are included.\n","    df_fact_joined = df_fact_source.join(\n","        df_dim_date_loaded,\n","        df_fact_source.FactDateKey == df_dim_date_loaded.DateKey,\n","        \"inner\"\n","    ).select(df_fact_source[\"*\"], df_dim_date_loaded[\"DateKey\"]) # Select `DateKey` from the dimension for the fact.\n","\n","    # Join with DimLocation: Link by Latitude, Longitude, and Place Description.\n","    # Important: When joining on multiple columns, especially strings, ensure handling of potential null values\n","    # (`(A.col IS NULL AND B.col IS NULL) OR (A.col = B.col)`) for robust matching.\n","    df_fact_joined = df_fact_joined.join(\n","        df_dim_location_loaded,\n","        (df_fact_joined.latitude == df_dim_location_loaded.Latitude) & \\\n","        (df_fact_joined.longitude == df_dim_location_loaded.Longitude) & \\\n","        ( (df_fact_joined.place.isNull() & df_dim_location_loaded.PlaceDescription.isNull()) | (df_fact_joined.place == df_dim_location_loaded.PlaceDescription) ),\n","        \"inner\"\n","    ).select(df_fact_joined[\"*\"], df_dim_location_loaded[\"LocationKey\"]) # Select `LocationKey`.\n","\n","    # Join with DimMagnitude: Link by `MagnitudeCategory`.\n","    df_fact_joined = df_fact_joined.join(\n","        df_dim_magnitude_loaded,\n","        df_fact_joined.magnitude_category == df_dim_magnitude_loaded.MagnitudeCategory,\n","        \"inner\"\n","    ).select(df_fact_joined[\"*\"], df_dim_magnitude_loaded[\"MagnitudeKey\"]) # Select `MagnitudeKey`.\n","\n","    # Join with DimEventType: Link by `EventType` and `MagType`.\n","    # Again, handle potential nulls in `magType` for complete matching.\n","    df_fact_joined = df_fact_joined.join(\n","        df_dim_event_type_loaded,\n","        (df_fact_joined.event_type == df_dim_event_type_loaded.EventType) & \\\n","        ( (df_fact_joined.magType.isNull() & df_dim_event_type_loaded.MagType.isNull()) | (df_fact_joined.magType == df_dim_event_type_loaded.MagType) ),\n","        \"inner\"\n","    ).select(df_fact_joined[\"*\"], df_dim_event_type_loaded[\"EventTypeKey\"]) # Select `EventTypeKey`.\n","\n","    # Generate `EarthquakeEventKey` for the fact table.\n","    # This serves as the primary key for each record in the fact table, generated by the ETL process.\n","    # `monotonically_increasing_id()` is suitable for full overwrite loads of the fact table.\n","    df_fact_joined = df_fact_joined.withColumn(\"EarthquakeEventKey\", F.monotonically_increasing_id() + 1)\n","\n","    # Select final columns for the fact table, aliasing them to match the desired Gold layer schema.\n","    # This step ensures the fact table has a clean, denormalized structure ready for analytics.\n","    # Ensure all columns designated as NOT NULL in the logical DW DDL are indeed non-null after joins/transformations.\n","    df_fact_final = df_fact_joined.select(\n","        F.col(\"EarthquakeEventKey\"),                             # Fact table primary key.\n","        F.col(\"event_id\").alias(\"EventID\"),                      # Business key from source.\n","        F.col(\"DateKey\"),                                        # Foreign Key to DimDate.\n","        F.col(\"TimeOfDayString\").cast(\"string\").alias(\"TimeOfDay\"), # Time component, cast to string to align with typical DW TIME(6) type.\n","        F.col(\"LocationKey\"),                                    # Foreign Key to DimLocation.\n","        F.col(\"MagnitudeKey\"),                                   # Foreign Key to DimMagnitude.\n","        F.col(\"EventTypeKey\"),                                   # Foreign Key to DimEventType.\n","        \n","        # Measures (aliased from Silver data for clarity)\n","        F.col(\"magnitude\").alias(\"Magnitude\"),\n","        F.col(\"depth_km\").alias(\"DepthKm\"),\n","        F.col(\"tsunami_warning\").alias(\"TsunamiWarning\"),\n","        F.col(\"significance\").alias(\"Significance\"),\n","        F.col(\"felt_reports\").alias(\"FeltReports\"),\n","        F.col(\"nst_stations\").alias(\"NumberOfStations\"),\n","        F.col(\"rms_travel_time\").alias(\"RmsTravelTime\"),\n","        F.col(\"gap_azimuthal\").alias(\"AzimuthalGap\"),\n","\n","        # Metadata/Audit columns (aliased from Silver or generated)\n","        F.col(\"url\").alias(\"SourceURL\"),\n","        F.col(\"silver_processing_timestamp_utc\").alias(\"SilverProcessingTimestampUTC\"),\n","        F.current_timestamp().cast(\"timestamp\").alias(\"DWLoadTimestampUTC\") # Timestamp of when this record was loaded into the Gold layer.\n","    ) \\\n","    .dropDuplicates([\"EventID\"]) # Ensure unique events by business key (`EventID`) before writing to fact table.\n","\n","    logger.info(f\"Fact table data prepped with {df_fact_final.count()} records after joins and deduplication.\")\n","    logger.info(\"Fact DataFrame Schema (Gold Layer):\")\n","    df_fact_final.printSchema()\n","    logger.info(\"Sample records from Fact Table:\")\n","    df_fact_final.show(5, truncate=False)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":10,"statement_ids":[10],"state":"finished","livy_statement_state":"available","session_id":"df1f360c-6673-44aa-b572-53f9c07ecffb","normalized_state":"finished","queued_time":"2025-07-31T21:55:00.2934207Z","session_start_time":null,"execution_start_time":"2025-07-31T21:56:05.1552881Z","execution_finish_time":"2025-07-31T21:56:19.6247077Z","parent_msg_id":"e19dac36-8a18-47cd-8cac-c23ce64961f4"},"text/plain":"StatementMeta(, df1f360c-6673-44aa-b572-53f9c07ecffb, 10, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stderr","text":["2025-07-31 21:56:05,631 - INFO - Loading dimension tables from Gold layer for fact table joins.\n2025-07-31 21:56:06,281 - INFO - Dimension tables loaded successfully from Gold layer for joining.\n2025-07-31 21:56:06,297 - INFO - Performing joins with dimension tables to resolve surrogate keys for the fact table.\n"]},{"output_type":"stream","name":"stdout","text":["root\n |-- EarthquakeEventKey: long (nullable = false)\n |-- EventID: string (nullable = true)\n |-- DateKey: long (nullable = true)\n |-- TimeOfDay: string (nullable = true)\n |-- LocationKey: long (nullable = true)\n |-- MagnitudeKey: long (nullable = true)\n |-- EventTypeKey: long (nullable = true)\n |-- Magnitude: double (nullable = true)\n |-- DepthKm: double (nullable = true)\n |-- TsunamiWarning: double (nullable = true)\n |-- Significance: integer (nullable = true)\n |-- FeltReports: integer (nullable = true)\n |-- NumberOfStations: integer (nullable = true)\n |-- RmsTravelTime: double (nullable = true)\n |-- AzimuthalGap: double (nullable = true)\n |-- SourceURL: string (nullable = true)\n |-- SilverProcessingTimestampUTC: timestamp (nullable = true)\n |-- DWLoadTimestampUTC: timestamp (nullable = false)\n\n+------------------+------------+--------+---------------+-----------+------------+------------+---------+-------+--------------+------------+-----------+----------------+-------------+------------+--------------------------------------------------------------+----------------------------+--------------------------+\n|EarthquakeEventKey|EventID     |DateKey |TimeOfDay      |LocationKey|MagnitudeKey|EventTypeKey|Magnitude|DepthKm|TsunamiWarning|Significance|FeltReports|NumberOfStations|RmsTravelTime|AzimuthalGap|SourceURL                                                     |SilverProcessingTimestampUTC|DWLoadTimestampUTC        |\n+------------------+------------+--------+---------------+-----------+------------+------------+---------+-------+--------------+------------+-----------+----------------+-------------+------------+--------------------------------------------------------------+----------------------------+--------------------------+\n|51539607568       |ak0249sbvq7l|20240731|04:05:01.839000|24680      |1           |4           |2.7      |99.9   |0.0           |112         |NULL       |NULL            |0.46         |NULL        |https://earthquake.usgs.gov/earthquakes/eventpage/ak0249sbvq7l|2025-07-30 18:52:51.51628   |2025-07-31 21:56:16.103496|\n|51539607553       |ak0249secrkx|20240731|08:20:43.858000|24656      |17179869185 |2           |4.6      |8.3    |1.0           |337         |28         |NULL            |0.5          |NULL        |https://earthquake.usgs.gov/earthquakes/eventpage/ak0249secrkx|2025-07-30 18:52:51.51628   |2025-07-31 21:56:16.103496|\n|51539607554       |ak0249sjvo15|20240731|17:48:41.869000|24644      |1           |4           |2.5      |51.1   |0.0           |96          |NULL       |NULL            |0.42         |NULL        |https://earthquake.usgs.gov/earthquakes/eventpage/ak0249sjvo15|2025-07-30 18:52:51.51628   |2025-07-31 21:56:16.103496|\n|51539607555       |ak0249slnopj|20240731|20:47:10.917000|25116      |1           |4           |2.5      |69.1   |0.0           |96          |NULL       |NULL            |0.42         |NULL        |https://earthquake.usgs.gov/earthquakes/eventpage/ak0249slnopj|2025-07-30 18:52:51.51628   |2025-07-31 21:56:16.103496|\n|51539607569       |ak0249sm72cx|20240731|21:37:52.697000|25117      |8589934593  |4           |3.8      |31.1   |0.0           |223         |3          |NULL            |0.75         |NULL        |https://earthquake.usgs.gov/earthquakes/eventpage/ak0249sm72cx|2025-07-30 18:52:51.51628   |2025-07-31 21:56:16.103496|\n+------------------+------------+--------+---------------+-----------+------------+------------+---------+-------+--------------+------------+-----------+----------------+-------------+------------+--------------------------------------------------------------+----------------------------+--------------------------+\nonly showing top 5 rows\n\n"]}],"execution_count":8,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"05153491-5b0a-4b5e-95aa-cdda546f72db"},{"cell_type":"code","source":["# --- Cell 9: Load Fact Table ---\n","\n","# This cell writes the prepared fact table data to the Gold layer of the Lakehouse.\n","# This is the final step in populating the Gold layer with earthquake events,\n","# making the data available for analytical consumption.\n","\n","# Get the count of records to write for logging. This avoids re-triggering a count action if not needed.\n","records_to_write_count = 0\n","if 'df_fact_final' in locals():\n","    records_to_write_count = df_fact_final.count()\n","\n","if records_to_write_count > 0: # Only attempt to write if there are records in the final DataFrame.\n","    try:\n","        # Write the Spark DataFrame (`df_fact_final`) to the `FactEarthquakeEvents` table\n","        # in the Gold layer of the Lakehouse. This will create a Delta table within\n","        # the 'Tables' section of your Lakehouse environment.\n","        #\n","        # `format(\"delta\")`: Specifies Delta Lake format for reliable, ACID-compliant data storage.\n","        # `mode(\"overwrite\")`: Replaces the entire fact table with the new data. This strategy is suitable\n","        #                      for full daily rebuilds, where all historical data is re-processed and\n","        #                      a fresh snapshot of the fact table is desired.\n","        #                      For incremental loads (e.g., only loading new events or updating existing ones),\n","        #                      a `MERGE INTO` operation (available in Delta Lake) would be the preferred approach\n","        #                      to perform upserts based on a business key.\n","        # `option(\"overwriteSchema\", \"true\")`: Allows the schema of the target Delta table to be updated\n","        #                                       if the incoming DataFrame's schema differs. Use with caution\n","        #                                       in production environments to avoid unintended schema changes,\n","        #                                       but it's common for iterative development or early stages.\n","        # `saveAsTable(...)`: Persists the data as a named Delta table in the Lakehouse's catalog,\n","        #                     making it easily discoverable and queryable via Spark SQL or other integrated tools.\n","        logger.info(f\"Writing {records_to_write_count} records to Gold fact table: {GOLD_FACT_EVENTS_TABLE}.\")\n","        df_fact_final.write \\\n","                     .format(\"delta\") \\\n","                     .mode(\"overwrite\") \\\n","                     .option(\"overwriteSchema\", \"true\") \\\n","                     .saveAsTable(GOLD_FACT_EVENTS_TABLE)\n","        \n","        logger.info(f\"FactEarthquakeEvents populated successfully with {records_to_write_count} records in the Gold layer table: {GOLD_FACT_EVENTS_TABLE}.\")\n","        \n","    except Exception as e:\n","        logger.error(f\"FATAL ERROR: An error occurred while saving data to Gold fact table '{GOLD_FACT_EVENTS_TABLE}': {e}\", exc_info=True)\n","        raise Exception(f\"Failed to populate Fact table in Gold layer: {e}\")\n","else:\n","    logger.warning(\"Skipping write to Gold fact table as the final DataFrame (`df_fact_final`) is empty. No data was available to be written.\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":11,"statement_ids":[11],"state":"finished","livy_statement_state":"available","session_id":"df1f360c-6673-44aa-b572-53f9c07ecffb","normalized_state":"finished","queued_time":"2025-07-31T21:55:00.3024766Z","session_start_time":null,"execution_start_time":"2025-07-31T21:56:19.6272357Z","execution_finish_time":"2025-07-31T21:56:27.7060981Z","parent_msg_id":"e4013325-d7ac-4e83-b0ad-a31cf8627567"},"text/plain":"StatementMeta(, df1f360c-6673-44aa-b572-53f9c07ecffb, 11, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stderr","text":["2025-07-31 21:56:20,887 - INFO - Writing 25132 records to Gold fact table: gold_earthquake_fact_earthquake_events.\n2025-07-31 21:56:27,236 - INFO - FactEarthquakeEvents populated successfully with 25132 records in the Gold layer table: gold_earthquake_fact_earthquake_events.\n"]}],"execution_count":9,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"3617ee08-08aa-4b5c-90bb-31c458a91145"},{"cell_type":"code","source":["# This cell provides a quick visual verification of the data written to the Gold fact table.\n","# It displays a sample of the records and the schema, confirming successful population.\n","if 'df_fact_final' in locals() and records_to_write_count > 0:\n","    try:\n","        # Ensure SparkSession is available to query the table.\n","        if 'spark' not in globals() or not isinstance(spark, SparkSession):\n","            logger.error(\"SparkSession 'spark' is not initialized for table verification. Attempting to get or create one.\")\n","            spark = SparkSession.builder.appName(\"GoldFactVerification\").getOrCreate()\n","            \n","        logger.info(f\"Displaying a sample of 5 records from the Gold fact table '{GOLD_FACT_EVENTS_TABLE}' for verification.\")\n","        # Retrieve the fact table as a Spark DataFrame and show its first 5 rows.\n","        # `truncate=False` ensures that column values are not truncated in the output.\n","        spark.table(GOLD_FACT_EVENTS_TABLE).show(5, truncate=False)\n","        \n","        logger.info(f\"Gold fact table '{GOLD_FACT_EVENTS_TABLE}' schema:\")\n","        spark.table(GOLD_FACT_EVENTS_TABLE).printSchema()\n","        logger.info(f\"Total records in Gold fact table '{GOLD_FACT_EVENTS_TABLE}': {spark.table(GOLD_FACT_EVENTS_TABLE).count()}\")\n","\n","    except Exception as e:\n","        logger.error(f\"An error occurred while trying to read and display data from Gold fact table '{GOLD_FACT_EVENTS_TABLE}': {e}\", exc_info=True)\n","else:\n","    logger.info(\"Skipping Gold fact table display as no data was processed and written to the table.\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":12,"statement_ids":[12],"state":"finished","livy_statement_state":"available","session_id":"df1f360c-6673-44aa-b572-53f9c07ecffb","normalized_state":"finished","queued_time":"2025-07-31T21:55:00.3046586Z","session_start_time":null,"execution_start_time":"2025-07-31T21:56:27.7082899Z","execution_finish_time":"2025-07-31T21:56:31.2640267Z","parent_msg_id":"38f0be75-20c5-4a0c-b581-903ac0173047"},"text/plain":"StatementMeta(, df1f360c-6673-44aa-b572-53f9c07ecffb, 12, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stderr","text":["2025-07-31 21:56:27,810 - INFO - Displaying a sample of 5 records from the Gold fact table 'gold_earthquake_fact_earthquake_events' for verification.\n2025-07-31 21:56:29,882 - INFO - Gold fact table 'gold_earthquake_fact_earthquake_events' schema:\n2025-07-31 21:56:30,501 - INFO - Total records in Gold fact table 'gold_earthquake_fact_earthquake_events': 25132\n"]}],"execution_count":10,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"28e98b20-a571-4454-ac8b-b443023c9690"},{"cell_type":"code","source":[],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"eecc2931-3e58-4082-8d78-8d79ff9efb72"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"default_lakehouse":"e01b7bb9-d1d5-46dc-9295-d1ec40fac90b","known_lakehouses":[{"id":"e01b7bb9-d1d5-46dc-9295-d1ec40fac90b"}],"default_lakehouse_name":"EarthquakeDataLakehouse","default_lakehouse_workspace_id":"e38b2c83-df95-47ed-9781-7d9946855c65"}}},"nbformat":4,"nbformat_minor":5}